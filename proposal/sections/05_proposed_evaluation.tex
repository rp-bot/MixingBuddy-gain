\section*{Proposed Evaluation}
\label{sec:proposed_evaluation}

The evaluation will assess the model's performance through human evaluation and automated metrics to address the research questions about effectiveness, learned conventions, and metric correlation.

\subsection*{Human Evaluation}
We will conduct a study with semi-professional audio engineers to evaluate the generated mixing advice.
They will evaluate mixing scenarios consisting of multi-track stems with designated anchor tracks and gain imbalances, along with the model's generated advice.
Participants will rate the advice on a Likert scale across five dimensions:
\begin{itemize}
    \item Effectiveness: How well the advice addresses the mixing problem
    \item Musicality: Appropriateness for the genre and musical context
    \item Conventions: Understanding of established mixing conventions
    \item Actionability: Clarity and ease of implementation
    \item Helpfulness: Real-world workflow utility
\end{itemize}

\subsection*{Automated Evaluation}
We will compute automated metrics to complement the human evaluation.
We will use LLM-as-a-Judge (GPT-4 evaluation with rubric) and semantic similarity metrics (BERTScore, ROUGE-L, METEOR).
For gain advice accuracy, we will measure direction accuracy (whether the advice correctly identifies if a track should be louder or quieter) and intensity accuracy (how well the suggested gain intensity changes match the optimal adjustments).
We will analyze the correlation between human ratings and automated metrics to understand how well objective measures align with human perception of mixing advice quality.



% \subsection{Ablation Studies}
% We will conduct systematic ablation studies to validate our approach:

% \paragraph{Architecture Ablations} Comparison with:
% \begin{itemize}
%     \item No-anchor conditioning baseline
%     \item Generic pre-trained ALM without fine-tuning
%     \item Different model architectures and fine-tuning strategies
% \end{itemize}

% \paragraph{Training Ablations} Evaluation of:
% \begin{itemize}
%     \item Impact of cross-sample training
%     \item Effect of data augmentation
%     \item Different annotation sources (programmatic vs. professional)
% \end{itemize}

