\section{Proposed Evaluation}
\label{sec:proposed_evaluation}

The evaluation of our anchor-conditioned Audio-Language Model (ALM) will be centered on human-centric assessments to directly address our research questions concerning the usefulness and musicality of the generated advice. 
We will employ a mixed-methods approach, combining qualitative feedback from audio professionals with quantitative ratings and automated metrics. 
The primary goal is to determine not just the technical accuracy of the advice, but its real-world value in a professional mixing context.

\subsection{Subjective Evaluation by Audio Professionals}
The core of our evaluation will be a formal listening study with a cohort of experienced audio engineers and producers.

\paragraph{Procedure} Participants will be presented with a series of mixing scenarios. 
Each scenario will consist of a set of multi-track stems, a designated anchor track, and a target track with a subtle gain imbalance. 
They will be shown the gain-balancing advice generated by our fine-tuned ALM for each scenario. 
Participants will then rate the advice on several key criteria.

\paragraph{Evaluation Criteria} Using a 7-point Likert scale, participants will rate the following:
\begin{itemize}
    \item \textbf{Effectiveness:} How well would this advice solve the given mixing problem?
    \item \textbf{Musicality:} Is the advice musically sensible and appropriate for the genre?
    \item \textbf{Actionability:} Is the advice clear, specific, and easy to implement in a DAW?
    \item \textbf{Helpfulness:} How helpful would this advice be in a real-world mixing session?
\end{itemize}

\paragraph{Qualitative Feedback} In addition to quantitative ratings, we will collect open-ended qualitative feedback. 
Participants will be asked to provide written comments on the strengths and weaknesses of the generated advice, its potential impact on their workflow, and any suggestions for improvement. 
This qualitative data will be analyzed using thematic analysis to identify recurring themes and provide deeper insights into the model's performance.

\subsection{Analysis of Learned Mixing Conventions}
To address the research question on learned conventions, a subset of the evaluation will focus on genre-specific scenarios. 
We will create test cases from different genres (e.g., pop, jazz, rock) and analyze whether the model's advice reflects established mixing norms for those genres (e.g., vocal level in pop vs. jazz). 
The ratings and qualitative feedback from the audio professionals on these specific scenarios will be used to assess the model's understanding of genre-specific expectations.

\subsection{Correlation of Subjective and Automated Metrics}
A key goal of this research is to understand the relationship between human perception and automated evaluation.

\paragraph{Automated Metrics} We will compute a suite of automated metrics for the generated advice, including:
\begin{itemize}
    \item \textbf{LLM-as-a-Judge:} A separate, powerful LLM (e.g., GPT-4) will be prompted to score the quality of the generated advice based on a detailed rubric.
    \item \textbf{Semantic Similarity Metrics:} We will calculate BERTScore, ROUGE-L, and METEOR by comparing the model's output to a reference text annotation.
\end{itemize}

\paragraph{Correlation Analysis} We will perform a statistical analysis to determine the correlation (e.g., using Pearson or Spearman correlation coefficients) between the subjective ratings from our human evaluation and the scores from the automated metrics. 
This analysis will help quantify how well current automated metrics align with human judgments of mixing advice, addressing a specific secondary research question.

\subsection{Ablation Studies}
To validate our architectural choices, we will conduct ablation studies comparing our proposed model to several baselines:
\begin{itemize}
    \item \textbf{No-Anchor Baseline:} A model trained without an explicit anchor track to assess the impact of the anchor-conditioning framework.
    \item \textbf{Generic ALM Baseline:} A general-purpose, pre-trained ALM (without fine-tuning) to measure the effectiveness of our task-specific training.
\end{itemize}
These baselines will be evaluated using the same subjective and automated protocols to quantify the performance gains of our proposed method.

