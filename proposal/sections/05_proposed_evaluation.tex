\section{Proposed Evaluation}
\label{sec:proposed_evaluation}

We will evaluate the proposed differential, multi-track ALM across objective, automated, and human-centered protocols, closely following and extending the evaluation strategies used by MixAssist \cite{Clemens_MarasoviÄ‡_2025}.

\subsection{Objective Differential-Mixing Accuracy}
\paragraph{Primary Metric: Differential Gain Error} For each evaluated input, we compute mean absolute error (MAE) between the model-inferred gain adjustments and the consensus reference ranges (Section~\ref{sec:proposed_method}). We report per-stem MAE (dB), overall MAE, and calibration curves across difficulty levels (magnitude of imbalance, number of perturbed stems).

\subsection{Automated LLM-as-a-Judge Assessments}
We adapt the LLM-as-a-judge protocol to the differential setting. Given: (i) the four-audio input bundle (A vocal, A backing, B vocal, B backing), (ii) the model's explanation, and (iii) a few-shot rubric describing good balance, we prompt a strong external LLM to score: (a) correctness of the stated adjustment (direction and magnitude), (b) reasoning specificity to audio evidence, and (c) safety (no overreach beyond gain). 

\subsection{Human Evaluation: Input-Output Assessment}
Music producers will evaluate the fine-tuned assistant's performance by reviewing input-output pairs. Participants will be presented with the four-audio input bundle (Version A vocal, Version A backing, Version B vocal, Version B backing) along with the model's generated explanation. They will complete a survey assessing the correctness of the stated adjustment, reasoning specificity to audio evidence, conversational naturalness, perceived creative contribution, and novelty of ideas, along with open-ended feedback.

\subsection{Manual Analysis of Audio Understanding Guidance}
We manually code a stratified sample of model outputs for: (i) reference to concrete audio cues (masking, spectral regions, temporal context), (ii) comparative reasoning (A vs. B alignment), and (iii) adherence to the differential task boundary. Two annotators label with Cohen's \(\kappa\) for agreement.

\subsection{Automated Semantic Evaluation Metrics}
For comparability with prior work, we compute BLEU, METEOR, ROUGE-L, and BERTScore between model explanations and reference annotations. Given multiple consensus references, we report max-over-refs and average-over-refs variants.

% \subsection{Ablations and Baselines}
% \begin{itemize}
%     \item \textbf{Single-Input vs. Differential:} Replace the four-input bundle with single Version~A only; measure performance drop.
%     \item \textbf{Text-Only Baseline:} Remove audio and provide only the task prompt; gauges label leakage and prompt priors.
%     \item \textbf{LoRA vs. QLoRA:} Compare PEFT variants in accuracy and efficiency (VRAM, throughput), keeping training steps fixed.
%     \item \textbf{Stem Subset Granularity:} Vocal-only vs. multi-stem settings; evaluate robustness to confounders.
%     \item \textbf{Instruction Style:} Terse vs. verbose prompts; assess sensitivity to prompt format.
% \end{itemize}


% \subsection{Statistical Analysis}
% We report bootstrap 95\% CIs for all metrics, perform paired tests where appropriate, and control for multiple comparisons. We release evaluation scripts and prompts to ensure reproducibility.

