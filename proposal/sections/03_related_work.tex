\section{Related Work}
\label{sec:related_work}

The field of ``Co-Creative AI'' provides a compelling framework, envisioning AI not as a replacement for human creativity, but as an intelligent partner that augments the artistic workflow \cite{Louie_Coenen_Huang_Terry_Cai_2020b, Tsiros_Palladini_2020}. 
Recent work has demonstrated the potential of co-creative systems in music production, showing positive user adoption and acceptance when AI tools provide appropriate levels of control and collaboration \cite{Bougueng_Tchemeube_Ens_Plut_Pasquier_Safi_Grabit_Rolland_2023, Vanka_Safi_Rolland_Fazekas_2023}. 

\subsection{Paradigms of Co-Creative AI in Audio}
Within the landscape of co-creative systems for audio engineering, early paradigms focused on automating the mixing process.
Initial approaches employed \textbf{rule-based methods} that relied on rule-based algorithms and expert knowledge systems to make mixing decisions, such as Dugan's pioneering automatic microphone mixer \cite{Dugan_1976} and knowledge-engineered autonomous mixing systems \cite{Perez-Gonzalez_Reiss_2013, Mansbridge_Finn_Reiss_2012}.
Recently, automated mixers evolved to function as \textbf{black box systems}, which take raw multitrack audio as input to produce a fully mixed output with minimal user intervention \cite{Steinmetz_Pons_Pascual_Serra_2020, Martinez-Ramirez_Liao_Fabbro_Uhlich_Nagashima_Mitsufuji_2022}.
While some of these systems provide a degree of user control through high-level parameters, they fundamentally abstract the underlying process, which limits the fine-grained control required in professional workflows.
In contrast, a more recent and promising paradigm is the \textbf{Language Bridge}, where natural language serves as the primary interface for interaction.
This approach allows users to articulate creative intent in descriptive terms, and our research is situated within this paradigm.
Recent work has begun exploring this direction across multiple dimensions: systems like MixAssist demonstrate the potential for audio-language models to provide contextual mixing advice through natural language dialogue \cite{Clemens_Marasović_2025}; Text2FX leverages CLAP embeddings to control audio effects through open-vocabulary natural language prompts \cite{Chu_OReilly_Barnett_Pardo_2025}; LLM2Fx shows that Large Language Models can predict audio effect parameters directly from textual descriptions in a zero-shot manner \cite{Doh_Koo_Martinez-Ramirez_Liao_Nam_Mitsufuji_2025}; and SonicMaster presents a unified generative model for music restoration and mastering controlled through natural language instructions \cite{Melechovsky_Mehrish_Herremans_2025}.
Additionally, research has explored speech recognition for mixer control \cite{Lai_Hung_Zhu_Wang_Sheu_Juang_2022}, natural language interfaces for audio production tools \cite{Pardo_Cartwright_Seetharaman_Kim_2019}, and word embeddings for automatic equalization \cite{Venkatesh_Moffat_Miranda_2022}.

\subsection{The Evolution of Language-Driven Models}
The development of language-driven models has progressed through several phases that have shaped how we interact with and control various modalities through natural language.
It began with \textbf{task-specific supervised models} for applications like audio tagging, which classified sounds into predefined categories \cite{kong2018attention}. 
A subsequent shift towards \textbf{representation learning} aimed to create more general-purpose audio embeddings to capture richer semantic information \cite{choi2017content}. 
The emergence of \textbf{contrastive dual-encoder models} like CLAP marked a significant advancement, learning audio concepts from natural language supervision and enabling zero-shot audio classification and retrieval \cite{Elizalde_Deshmukh_Ismail_Wang_2023, Koepke_Oncescu_Henriques_Akata_Albanie_2023}. 
The rise of deep \textbf{generative models} like GANs and VAEs enabled audio synthesis and transformation, though precise control remained a challenge \cite{donahue2018wavegan, engel2019disentangled}. 
More recently, \textbf{generative and diffusion-based multimodal models} have emerged, leveraging the power of diffusion models and large language models for text-to-audio generation \cite{Liu_Yuan_Liu_Mei_Kong_Tian_Wang_Wang_Wang_Plumbley_2024, Xue_Deng_Gao_Li_2024}. 
The development of \textbf{Large Audio-Language Models (LALMs)} and instruction-following systems represents an important advancement, with models like AudioLM demonstrating language modeling approaches to audio generation \cite{Borsos_Marinier_Vincent_Kharitonov_Pietquin_Sharifi_Roblek_Teboul_Grangier_Tagliasacchi_etal_2023}. 
The language understanding capabilities of these models, when combined with audio processing abilities, provide the foundation for the Language Bridge paradigm, enabling connections between user intent and audio manipulation \cite{zhao2023survey}.

\subsection{Multimodal Large Language Models}
The development of Multimodal Large Language Models (MM-LLMs) has advanced AI capabilities, enabling models to process and understand multiple modalities simultaneously. Several architectural approaches have emerged:

\textbf{Unified Multimodal Models} like Unified-IO \cite{lu2022unified} are trained from the ground up on datasets spanning multiple modalities, demonstrating unified understanding across vision, language, and audio domains.

\textbf{Modality Interface Architectures} augment pre-trained, text-only LLMs with specialized encoders for other modalities. Examples include Qwen-Audio \cite{Chu_Xu_Zhou_Yang_Zhang_Yan_Zhou_Zhou_2023} and SALMONN \cite{Tang_Yu_Sun_Chen_Tan_Li_Lu_Ma_Zhang_2024}, which integrate speech and audio encoders with LLMs.

\textbf{Any-to-Any Multimodal Systems} enable models to both perceive and generate content across multiple modalities. NExT-GPT \cite{Wu_Fei_Qu_Ji_Chua_2024} connects LLMs with multimodal adaptors and diffusion decoders, while Audiobox \cite{Vyas_Shi_Le_Tjandra_Wu_Guo_Zhang_Zhang_Adkins_Ngan_eta_2023} provides audio generation capabilities across speech, sound, and music through natural language prompts.

\textbf{Instruction-Following Multimodal Models} like Macaw-LLM \cite{Lyu_Wu_Wang_Huang_Liu_Du_Shi_Tu_2023} and PandaGPT \cite{Su_Lan_Li_Xu_Wang_Cai_2023} focus on following instructions across modalities for multi-turn dialogue scenarios.

\subsection{Audio-Language Model Applications}
The application of fine-tuning to MM-LLMs has enabled diverse capabilities across audio domains. Rather than organizing by domain (general audio, music, speech), we categorize by functionality to highlight common patterns and reduce redundancy.

\subsubsection{Audio Understanding and Analysis}
Models across domains employ LLMs as backbones for audio analysis: LTU \cite{Gong_Luo_Liu_Karlinsky_Glass_2024}, SALMONN \cite{Tang_Yu_Sun_Chen_Tan_Li_Lu_Ma_Zhang_2024}, Qwen-Audio \cite{Chu_Xu_Zhou_Yang_Zhang_Yan_Zhou_Zhou_2023}, and UNIFIED-IO 2 \cite{Lu_Clark_Lee_Zhang_Khosla_Marten_Hoiem_Kembhavi_2024} for environmental sounds; Music Understanding LLaMA (MU-LLaMA) \cite{Liu_Hussain_Sun_Shan_2024}, LLARK \cite{Gardner_Durand_Stoller_Bittner_2024}, MusicAgent \cite{Yu_Song_Lu_He_Tan_Ye_Zhang_Bian_2023}, and LyricWhiz \cite{Zhuo_Yuan_Pan_Ma_LI_Zhang_Liu_Dannenberg_Fu_Lin_etal_2024} for music analysis; and SpeechGPT \cite{Zhang_Li_Zhang_Zhan_Wang_Zhou_Qiu_2023}, AudioPaLM \cite{Rubenstein_Asawaroengchai_Nguyen_Bapna_Borsos_Quitry_Chen_Badawy_Han_Kharitonov_etal_2023}, and Speech-LLaMA \cite{Wu_Gaur_Chen_Zhou_Zhu_Wang_Li_Liu_Ren_Liu_etal_2023} for speech processing, with additional works improving ASR accuracy through in-context learning \cite{Wang_Yang_Wu_Zhang_2024, Yu_Tang_Sun_Chen_Tan_Li_Lu_Ma_Zhang_2024}. Tool-coordinating interfaces like AudioGPT \cite{Huang_Li_Yang_Shi_Chang_Ye_Wu_Hong_Huang_Liu_etal_2023} and HuggingGPT \cite{Shen_Song_Tan_Li_Lu_Zhuang_2023} provide unified access to specialized audio understanding capabilities. Recent work has improved automated audio captioning by integrating pretrained models with LLMs \cite{Wu_Donahue_Watanabe_Bryan_2024}.

\subsubsection{Audio Generation and Synthesis}
Generation approaches employ diverse techniques across domains: \textbf{Text-to-Audio} models like TANGO \cite{Ghosal_Majumder_Mehrish_Poria_2023} (FLAN-T5 embedders), Make-an-Audio 2 \cite{Huang_Ren_Huang_Yang_Ye_Zhang_Liu_Yin_Ma_Zhao_2023} (latent diffusion), and WavJourney \cite{Liu_Zhu_Liu_Yuan_Huang_Cui_Liang_Cao_Kong_Plumbley_etal_2025} (LLM agents); \textbf{Music Generation} including MusicLM \cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_etal_2023} and Jukebox \cite{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020} (Transformer architectures), MusicGen \cite{Copet_Kreuk_Gat_Remez_Kant_Synnaeve_Adi_Défossez} and Music ControlNet \cite{Wu_Donahue_Watanabe_Bryan_2024} (LLM text embedders), and ChatMusician \cite{Yuan_Lin_Wang_Tian_Wu_Shen_Zhang_Wu_Liu_Zhou_etal_2024} and SongComposer \cite{Ding_Liu_Dong_Zhang_Qian_Huang_He_Lin_Wang_2025} (symbolic generation); and \textbf{Speech Synthesis} with VALL-E \cite{Wang_Chen_Wu_Zhang_Zhou_Liu_Chen_Liu_Wang_Li_etal_2023} (neural codec language models), LLaMA/OPT integrations with VALL-E \cite{Hao_Zhou_Liu_Li_Hu_Wang_Wei_2025}, and LauraGPT \cite{Du_Wang_Chen_Chu_Gao_Li_Hu_Zhou_Xu_Ma_etal_2024} (unified GPT for TTS). Research also explores word surprisal for improved speech synthesis prosody \cite{Kakouros_imko_Vainio_Suni_2023}.

\subsubsection{Audio Editing and Manipulation}
Audio editing and manipulation capabilities have been demonstrated through systems like Loop Copilot \cite{Zhang_Maezawa_Xia_Yamamoto_Dixon_2024}, which integrates LLMs with specialized AI music models to enable conversational music loop creation and editing workflows.

\subsection{How LLMs are Utilized in Audio Tasks}
The integration of LLMs in audio tasks can be categorized into several approaches:

\begin{itemize}
    \item \textbf{LLMs as Backbone:} Pre-trained LLMs (e.g., LLaMA) serve as the central architecture, either with modality-specific encoders/decoders (cascade approach) or by tokenizing raw audio into discrete tokens for direct LLM input (unified approach).
    \item \textbf{LLMs as Conditioner:} LLMs encode text prompts into embeddings that condition the audio generation process.
    \item \textbf{LLMs as Labeller:} LLMs are used to convert class labels from large audio datasets into full-sentence audio descriptions or captions, often utilizing self-instruction techniques.
    \item \textbf{LLMs as Agent:} LLMs act as controllers, interfacing with and coordinating external tools to accomplish audio tasks.
    \item \textbf{LLMs Inspired Backbone:} This approach discretizes audio into tokens for next-token prediction, targeting LLM-like capabilities in audio.
\end{itemize}

Additionally, tool-augmented multimodal agents like ControlLLM \cite{Liu_Lai_Gao_Cui_Li_Zhu_Lu_Chen_Qiao_Dai_etal_2025}, ModelScope-Agent \cite{Li_Chen_Yan_Shen_Xu_Wu_Zhang_Zhou_Chen_Cheng_etal_2023}, and HuggingGPT \cite{Shen_Song_Tan_Li_Lu_Zhuang_2023} can generate speech and music by invoking specialized audio tools.