\section*{Proposed Method}
\label{sec:proposed_method}

This research presents a framework for fine-tuning an Audio-Language Model (ALM) to generate gain-balancing advice for music mixing. 
The approach conditions the model on a designated anchor track to learn relational understanding of multi-track audio levels.

\subsection*{Data Preprocessing and Dataset Creation}
We will utilize the MUSDB18 dataset \cite{Rafii_Liutkus_St√∂ter_Mimilakis_Bittner_2019} to create a structured training dataset in JSONL format.

Each training instance will contain an instruction (contextual description of the current clip and stems), audio data with an anchor track and other track stems, and a response with generated mixing advice describing gain relationships. Since the original tracks are already well-balanced, we will create flawed versions by artificially adjusting gain levels at training time.

\paragraph{Collecting dataset statistics.}
We analyze MUSDB18HQ tracks by splitting each song into non-overlapping 10~s windows and computing per-stem and mixture loudness/activity features: root-mean-square (RMS) in dBFS and active frame ratio using 50~ms windows. We summarize distributions per split and stem (p5, p25, p50, p75, p95) to obtain robust operating ranges. These statistics guide gating thresholds for filtering silent or near-silent segments and inform track-level variability.

\paragraph{Thresholds and preprocessing.}
Guided by the observed distributions, we adopt conservative gates to exclude low-information segments: mixture minimum RMS at about $-25$~dBFS, stem minimum RMS at about $-40$~dBFS, and a minimum active-frame ratio of 0.30. We chunk audio into 10~s segments, discard tails shorter than 5~s, and select an \emph{anchor} stem per chunk by preference order (bass$\rightarrow$drums$\rightarrow$vocals$\rightarrow$other) subject to the activity gates. We store relative paths to stems (no audio duplication), time bounds (start/end), and per-chunk activity snapshots in a JSONL \texttt{metadata} file.

\paragraph{Track-wide error labeling via IQR-scaled policy.}
To synthesize target corrections that are consistent with each song's context, we pool all stems per track to compute a track-level median RMS (dBFS) and interquartile range (IQR). For each chunk, we sample an error category with priors (\textit{no\_error, quiet, very\_quiet, loud, very\_loud}). We then map categories to an \emph{adaptive} target level using an IQR-scaled policy: for attenuation categories we shift the target below the track median by $\alpha\cdot\text{IQR}$ (e.g., $\alpha\in\{0.75,1.50\}$), and for amplification categories we shift above the median by the same scaled amount. The per-chunk intended gain (in dB) is the clamped difference between this target and the current chunk's stem RMS. We store error labels (target stem, category, intended gain) keyed by chunk identifier.

\paragraph{Synthesis of instructions and responses.}
For this project scope, we focus on audio-conditioned training with a constant instruction family and diverse response templates. Instructions concisely describe the clip duration, available stems, and the anchor stem. Responses are selected from a small template bank per category and instantiated with placeholders (e.g., target stem and intended gain). The final SFT record comprises an \texttt{instruction}, a \texttt{response}, and a \texttt{meta} block with references to audio paths, time bounds, selected anchor/target stems, and activity summaries. During training, we inject the labeled error \emph{on-the-fly} by applying the intended gain to the target stem and summing stems to form a flawed mixture, aligning the audio conditioning with the response content.

While future work considers multiple prompt categories, in this project we concentrate on response-only mixing guidance with a constant instruction context. For completeness, we outline possible prompt categories for subsequent extensions:
\begin{itemize}
    \item \textbf{Audio-only prompts:} Training instances with no text input, requiring the model to analyze audio stems directly.
    
    \item \textbf{Incorrect description prompts:} Training instances where the text prompt contains deliberately wrong descriptions of the audio content (e.g., ``The vocals are too quiet'' when they are actually too loud).
    
    \item \textbf{Advice-seeking prompts:} Training instances with open-ended questions or requests for mixing guidance (e.g., ``How should I balance these tracks?'').
    
    \item \textbf{Correct description prompts:} Training instances with accurate text descriptions of the audio content (e.g., ``The vocal stem is too loud relative to the drums'' or ``The drums are too dominant and overpowering'').
\end{itemize}
We will focus on a single anchor track approach and experiment with scenarios: balanced mixes, one flawed track, and multiple flawed tracks requiring correction.

\subsection*{Model Architecture and Training Strategy}
We will use Qwen-Audio \cite{Chu_Xu_Zhou_Yang_Zhang_Yan_Zhou_Zhou_2023} as the base model, while exploring other pre-trained Audio-Language Models for fine-tuning.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/arch.pdf}
    \caption{Proposed architecture of Audio-Language Model fine-tuning for music mixing advice generation.}
    \label{fig:architecture}
\end{figure*}
We will use Low-Rank Adaptation (LoRA) \cite{Hu_Shen_Wallis_Allen-Zhu_Li_Wang_Wang_Chen_2021} and QLoRA \cite{Dettmers_2023_QLoRA} for parameter-efficient fine-tuning.

\subsection*{Data Augmentation and Cross-Sample Training}
We will implement cross-sample training strategies to improve generalization.
We will explore creating hybrid training examples by combining stems from different songs while maintaining musical coherence.

% \subsection*{Architecture Experiments}
% We will conduct systematic experiments to optimize the model architecture and training approach.
% We will evaluate different anchor selection strategies and their impact on model performance across various musical contexts and genres.
