@inproceedings{Bougueng_Tchemeube_Ens_Plut_Pasquier_Safi_Grabit_Rolland_2023,
	title        = {Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition},
	author       = {Bougueng Tchemeube, Renaud and Ens, Jeffrey and Plut, Cale and Pasquier, Philippe and Safi, Maryam and Grabit, Yvan and Rolland, Jean-Baptiste},
	year         = 2023,
	month        = aug,
	booktitle    = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	address      = {Macau, SAR China},
	pages        = {5769–5778},
	doi          = {10.24963/ijcai.2023/640},
	isbn         = {978-1-956792-03-4},
	url          = {https://www.ijcai.org/proceedings/2023/640},
	abstractnote = {With the rise of artificial intelligence (AI), there has been increasing interest in human-AI co-creation in a variety of artistic domains including music as AI-driven systems are frequently able to generate human-competitive artifacts. Now, the implications of such systems for musical practice are being investigated. We report on a thorough evaluation of the user adoption of the Multi-Track Music Machine (MMM) as a co-creative AI tool for music composers. To do this, we integrate MMM into Cubase, a popular Digital Audio Workstation (DAW) by Steinberg, by producing a “1-parameter” plugin interface named MMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a methodological assemblage as a 3-part mixed method study measuring usability, user experience and technology acceptance of the system across two groups of expert-level composers: hobbyists and professionals. Results show positive usability and acceptance scores. Users report experiences of novelty, surprise and ease of use from using the system, and limitations on controllability and predictability of the interface when generating music. Findings indicate no significant difference between the two user groups.},
	language     = {en}
}
@article{Clemens_Marasović_2025,
	title        = {MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing},
	author       = {Clemens, Michael and Marasović, Ana},
	year         = 2025,
	month        = jul,
	publisher    = {arXiv},
	number       = {arXiv:2507.06329},
	doi          = {10.48550/arXiv.2507.06329},
	url          = {http://arxiv.org/abs/2507.06329},
	note         = {arXiv:2507.06329 [cs]},
	abstractnote = {While AI presents significant potential for enhancing music mixing and mastering workflows, current research predominantly emphasizes end-to-end automation or generation, often overlooking the collaborative and instructional dimensions vital for co-creative processes. This gap leaves artists, particularly amateurs seeking to develop expertise, underserved. To bridge this, we introduce MixAssist, a novel audio-language dataset capturing the situated, multi-turn dialogue between expert and amateur music producers during collaborative mixing sessions. Comprising 431 audio-grounded conversational turns derived from 7 in-depth sessions involving 12 producers, MixAssist provides a unique resource for training and evaluating audio-language models that can comprehend and respond to the complexities of real-world music production dialogues. Our evaluations, including automated LLM-as-a-judge assessments and human expert comparisons, demonstrate that fine-tuning models such as Qwen-Audio on MixAssist can yield promising results, with Qwen significantly outperforming other tested models in generating helpful, contextually relevant mixing advice. By focusing on co-creative instruction grounded in audio context, MixAssist enables the development of intelligent AI assistants designed to support and augment the creative process in music mixing.}
}
@article{Louie_Coenen_Huang_Terry_Cai_2020a,
	title        = {Cococo: AI-Steering Tools for Music Novices Co-Creating with Generative Models1},
	author       = {Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry, Michael and Cai, Carrie J},
	year         = 2020,
	abstractnote = {In this work1, we investigate how novices co-create music with a deep generative model, and what types of interactive controls are important for an effective co-creation experience. Through a needfinding study, we found that generative AI can overwhelm novices when the AI generates too much content, and can make it hard to express creative goals when outputs appear to be random. To better match co-creation needs, we built Cococo, a music editor web interface that adds interactive capabilities via a set of AIsteering tools. These tools restrict content generation to particular voices and time measures, and help to constrain non-deterministic output to specific high-level directions. We found that the tools helped users increase their control, self-efficacy, and creative ownership, and we describe how the tools affected novices’ strategies for composing and managing their interaction with AI.},
	language     = {en}
}
@inproceedings{Louie_Coenen_Huang_Terry_Cai_2020b,
	title        = {Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models},
	author       = {Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry, Michael and Cai, Carrie J.},
	year         = 2020,
	month        = apr,
	booktitle    = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	publisher    = {ACM},
	address      = {Honolulu HI USA},
	pages        = {1–13},
	doi          = {10.1145/3313831.3376739},
	isbn         = {978-1-4503-6708-0},
	url          = {https://dl.acm.org/doi/10.1145/3313831.3376739},
	language     = {en}
}
@misc{Tsiros_Palladini_2020,
	title        = {Towards a Human-Centric Design Framework for AI Assisted Music Production},
	author       = {Tsiros, Augoustinos and Palladini, Alessandro},
	year         = 2020,
	month        = jun,
	journal      = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	publisher    = {Zenodo},
	pages        = {399--404},
	doi          = {10.5281/zenodo.4813436},
	url          = {https://zenodo.org/records/4813436},
	abstractnote = {In this paper, we contribute to the discussion on how to best design human-centric MIR tools for live audio mixing by bridging the gap between research on complex systems, the psychology of automation and the design of tools that support creativity in music production. We present the design of the Channel-AI, an embedded AI system which performs instrument recognition and generates parameter settings suggestions for gain levels, gating, compression and equalization which are specific to the input signal and the instrument type. We discuss what we believe to be the key design principles and perspectives on the making of intelligent tools for creativity and for experts in the loop. We demonstrate how these principles have been applied to inform the design of the interaction between expert live audio mixing engineers with the Channel-AI (i.e. a corpus of AI features embedded in the Midas HD Console. We report the findings from a preliminary evaluation we conducted with three professional mixing engineers and reflect on mixing engineers’ comments about the Channel-AI on social media.}
}
@inproceedings{Vanka_Rolland_Fazekas_2025,
	title        = {Demonstrating Diff-MSTC: A Controllable and Context-Aware AI System for Multitrack Mixing in Digital Audio Workstation Cubase},
	author       = {Vanka, Soumya Sai and Rolland, Jean-Baptiste and Fazekas, György},
	year         = 2025,
	month        = apr,
	booktitle    = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
	publisher    = {ACM},
	address      = {Yokohama Japan},
	pages        = {1–5},
	doi          = {10.1145/3706599.3721167},
	isbn         = {979-8-4007-1395-8},
	url          = {https://dl.acm.org/doi/10.1145/3706599.3721167},
	language     = {en}
}
@article{Vanka_Safi_Rolland_Fazekas_2023,
	title        = {Adoption of AI Technology in the Music Mixing Workflow: An Investigation},
	author       = {Vanka, Soumya Sai and Safi, Maryam and Rolland, Jean-Baptiste and Fazekas, George},
	year         = 2023,
	month        = sep,
	publisher    = {arXiv},
	number       = {arXiv:2304.03407},
	doi          = {10.48550/arXiv.2304.03407},
	url          = {http://arxiv.org/abs/2304.03407},
	note         = {arXiv:2304.03407 [cs]},
	abstractnote = {The integration of artificial intelligence (AI) technology in the music industry is driving a significant change in the way music is being composed, produced and mixed. This study investigates the current state of AI in the mixing workflows and its adoption by different user groups. Through semi-structured interviews, a questionnaire-based study, and analyzing web forums, the study confirms three user groups comprising amateurs, pro-ams, and professionals. Our findings show that while AI mixing tools can simplify the process and provide decent results for amateurs, pro-ams seek precise control and customization options, while professionals desire control and customization options in addition to assistive and collaborative technologies. The study provides strategies for designing effective AI mixing tools for different user groups and outlines future directions.}
}
