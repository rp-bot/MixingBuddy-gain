@article{zhao2023survey,
	title        = {A survey of large language models},
	author       = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2303.18223}
}
@inproceedings{lu2022unified,
	title        = {Unified-io: A unified model for vision, language, and audio},
	author       = {Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
	year         = 2022,
	booktitle    = {European Conference on Computer Vision},
	pages        = {525--542},
	organization = {Springer}
}
@article{borsos2022audiolm,
	title        = {Audiolm: a language modeling approach to audio generation},
	author       = {Borsos, Zal{\'a}n and Frank, Raphael and Zeghidour, Neil and Chan, Yanqi and Jansen, Aren and Henderson, Peter and Tagliasacchi, Marco and Sharifi, Mohammad},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.03143}
}
@article{schick2023toolformer,
	title        = {Toolformer: Language models that teach themselves to use tools},
	author       = {Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.04761}
}
@article{thummala2022lamda,
	title        = {LaMDA: Language Models for Dialog Applications},
	author       = {Romal Thummala, et al.},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2201.08239}
}
@article{Chu_Xu_Zhou_Yang_Zhang_Yan_Zhou_Zhou_2023,
	title        = {Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},
	author       = {Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren},
	year         = 2023,
	month        = dec,
	publisher    = {arXiv},
	number       = {arXiv:2311.07919},
	doi          = {10.48550/arXiv.2311.07919},
	url          = {http://arxiv.org/abs/2311.07919},
	note         = {arXiv:2311.07919 [eess]},
	abstractnote = {Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.}
}
@article{Elizalde_Deshmukh_Wang_2024,
	title        = {Natural Language Supervision for General-Purpose Audio Representations},
	author       = {Elizalde, Benjamin and Deshmukh, Soham and Wang, Huaming},
	year         = 2024,
	month        = feb,
	publisher    = {arXiv},
	number       = {arXiv:2309.05767},
	doi          = {10.48550/arXiv.2309.05767},
	url          = {http://arxiv.org/abs/2309.05767},
	note         = {arXiv:2309.05767 [cs]},
	abstractnote = {Audio-Language models jointly learn multimodal text and audio representations that enable Zero-Shot inference. Models rely on the encoders to create powerful representations of the input and generalize to multiple tasks ranging from sounds, music, and speech. Although models have achieved remarkable performance, there is still a performance gap with task-specific models. In this paper, we propose a Contrastive Language-Audio Pretraining model that is pretrained with a diverse collection of 4.6M audio-text pairs employing two innovative encoders for Zero-Shot inference. To learn audio representations, we trained an audio encoder on 22 audio tasks, instead of the standard training of sound event classification. To learn language representations, we trained an autoregressive decoder-only model instead of the standard encoder-only models. Then, the audio and language representations are brought into a joint multimodal space using Contrastive Learning. We used our encoders to improve the downstream performance by a margin. We extensively evaluated the generalization of our representations on 26 downstream tasks, the largest in the literature. Our model achieves state of the art results in several tasks leading the way towards general-purpose audio representations.}
}
@article{Lyu_Wu_Wang_Huang_Liu_Du_Shi_Tu_2023,
	title        = {Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration},
	author       = {Lyu, Chenyang and Wu, Minghao and Wang, Longyue and Huang, Xinting and Liu, Bingshuai and Du, Zefeng and Shi, Shuming and Tu, Zhaopeng},
	year         = 2023,
	month        = jun,
	publisher    = {arXiv},
	number       = {arXiv:2306.09093},
	doi          = {10.48550/arXiv.2306.09093},
	url          = {http://arxiv.org/abs/2306.09093},
	note         = {arXiv:2306.09093 [cs]},
	abstractnote = {Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.}
}
@article{Su_Lan_Li_Xu_Wang_Cai_2023,
	title        = {PandaGPT: One Model To Instruction-Follow Them All},
	author       = {Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	number       = {arXiv:2305.16355},
	doi          = {10.48550/arXiv.2305.16355},
	url          = {http://arxiv.org/abs/2305.16355},
	note         = {arXiv:2305.16355 [cs]},
	abstractnote = {We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an initial step toward building AGI that can perceive and understand inputs in different modalities holistically, as we humans do. Our project page is at https://panda-gpt.github.io/.}
}
@article{Tang_Yu_Sun_Chen_Tan_Li_Lu_Ma_Zhang_2024,
	title        = {SALMONN: Towards Generic Hearing Abilities for Large Language Models},
	author       = {Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
	year         = 2024,
	month        = apr,
	publisher    = {arXiv},
	number       = {arXiv:2310.13289},
	doi          = {10.48550/arXiv.2310.13289},
	url          = {http://arxiv.org/abs/2310.13289},
	note         = {arXiv:2310.13289 [cs]},
	abstractnote = {Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.}
}
@article{Vyas_Shi_Le_Tjandra_Wu_Guo_Zhang_Zhang_Adkins_Ngan_eta_2023,
	title        = {Audiobox: Unified Audio Generation with Natural Language Prompts},
	author       = {Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and Wang, Jeff and Cruz, Ivan and Akula, Bapi and Akinyemi, Akinniyi and Ellis, Brian and Moritz, Rashel and Yungster, Yael and Rakotoarison, Alice and Tan, Liang and Summers, Chris and Wood, Carleigh and Lane, Joshua and Williamson, Mary and Hsu, Wei-Ning},
	year         = 2023,
	month        = dec,
	publisher    = {arXiv},
	number       = {arXiv:2312.15821},
	doi          = {10.48550/arXiv.2312.15821},
	url          = {http://arxiv.org/abs/2312.15821},
	note         = {arXiv:2312.15821 [cs]},
	abstractnote = {Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like “a person speaking” and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/}
}
@inproceedings{Wu_Fei_Qu_Ji_Chua_2024,
	title        = {NExT-GPT: Any-to-Any Multimodal LLM},
	author       = {Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
	year         = 2024,
	month        = jun,
	url          = {https://openreview.net/forum?id=NZQkumsNlf},
	abstractnote = {While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio. By leveraging the existing well-trained high-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.},
	language     = {en}
}
@article{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_etal_2023,
	title        = {MusicLM: Generating Music From Text},
	author       = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zalán and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
	year         = 2023,
	month        = jan,
	publisher    = {arXiv},
	number       = {arXiv:2301.11325},
	doi          = {10.48550/arXiv.2301.11325},
	url          = {http://arxiv.org/abs/2301.11325},
	note         = {arXiv:2301.11325 [cs]},
	abstractnote = {We introduce MusicLM, a model generating high-fidelity music from text descriptions such as “a calming violin melody backed by a distorted guitar riff”. MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.}
}
@article{Copet_Kreuk_Gat_Remez_Kant_Synnaeve_Adi_Défossez,
	title        = {Simple and Controllable Music Generation},
	author       = {Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and Défossez, Alexandre},
	abstractnote = {We tackle the task of conditional music generation. We introduce MUSICGEN, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MUSICGEN is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MUSICGEN can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MUSICGEN. Music samples, code, and models are available at github.com/facebookresearch/audiocraft.},
	language     = {en}
}
@article{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020,
	title        = {Jukebox: A Generative Model for Music},
	author       = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	year         = 2020,
	month        = apr,
	publisher    = {arXiv},
	number       = {arXiv:2005.00341},
	doi          = {10.48550/arXiv.2005.00341},
	url          = {http://arxiv.org/abs/2005.00341},
	note         = {arXiv:2005.00341 [eess]},
	abstractnote = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox}
}
@article{Ding_Liu_Dong_Zhang_Qian_Huang_He_Lin_Wang_2025,
	title        = {SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition},
	author       = {Ding, Shuangrui and Liu, Zihan and Dong, Xiaoyi and Zhang, Pan and Qian, Rui and Huang, Junhao and He, Conghui and Lin, Dahua and Wang, Jiaqi},
	year         = 2025,
	month        = may,
	publisher    = {arXiv},
	number       = {arXiv:2402.17645},
	doi          = {10.48550/arXiv.2402.17645},
	url          = {http://arxiv.org/abs/2402.17645},
	note         = {arXiv:2402.17645 [cs]},
	abstractnote = {Creating lyrics and melodies for the vocal track in a symbolic format, known as song composition, demands expert musical knowledge of melody, an advanced understanding of lyrics, and precise alignment between them. Despite achievements in sub-tasks such as lyric generation, lyric-to-melody, and melody-to-lyric, etc, a unified model for song composition has not yet been achieved. In this paper, we introduce SongComposer, a pioneering step towards a unified song composition model that can readily create symbolic lyrics and melodies following instructions. SongComposer is a music-specialized large language model (LLM) that, for the first time, integrates the capability of simultaneously composing lyrics and melodies into LLMs by leveraging three key innovations: 1) a flexible tuple format for word-level alignment of lyrics and melodies, 2) an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge to capture rhythm, and 3) a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence. Extensive experiments demonstrate that SongComposer outperforms advanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation. Moreover, we will release SongCompose, a large-scale dataset for training, containing paired lyrics and melodies in Chinese and English.}
}
@article{Du_Wang_Chen_Chu_Gao_Li_Hu_Zhou_Xu_Ma_etal_2024,
	title        = {LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT},
	author       = {Du, Zhihao and Wang, Jiaming and Chen, Qian and Chu, Yunfei and Gao, Zhifu and Li, Zerui and Hu, Kai and Zhou, Xiaohuan and Xu, Jin and Ma, Ziyang and Wang, Wen and Zheng, Siqi and Zhou, Chang and Yan, Zhijie and Zhang, Shiliang},
	year         = 2024,
	month        = jul,
	publisher    = {arXiv},
	number       = {arXiv:2310.04673},
	doi          = {10.48550/arXiv.2310.04673},
	url          = {http://arxiv.org/abs/2310.04673},
	note         = {arXiv:2310.04673 [cs]},
	abstractnote = {Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.}
}
@article{Gardner_Durand_Stoller_Bittner_2024,
	title        = {LLark: A Multimodal Instruction-Following Language Model for Music},
	author       = {Gardner, Josh and Durand, Simon and Stoller, Daniel and Bittner, Rachel M.},
	year         = 2024,
	month        = jun,
	publisher    = {arXiv},
	number       = {arXiv:2310.07160},
	doi          = {10.48550/arXiv.2310.07160},
	url          = {http://arxiv.org/abs/2310.07160},
	note         = {arXiv:2310.07160 [cs]},
	abstractnote = {Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for emph{music} understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, reasoning), we show that LLark matches or outperforms existing baselines in music understanding, and that humans show a high degree of agreement with its responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark .}
}
@article{Ghosal_Majumder_Mehrish_Poria_2023,
	title        = {Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model},
	author       = {Ghosal, Deepanway and Majumder, Navonil and Mehrish, Ambuj and Poria, Soujanya},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	number       = {arXiv:2304.13731},
	doi          = {10.48550/arXiv.2304.13731},
	url          = {http://arxiv.org/abs/2304.13731},
	note         = {arXiv:2304.13731 [eess]},
	abstractnote = {The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.}
}
@article{Gong_Luo_Liu_Karlinsky_Glass_2024,
	title        = {Listen, Think, and Understand},
	author       = {Gong, Yuan and Luo, Hongyin and Liu, Alexander H. and Karlinsky, Leonid and Glass, James},
	year         = 2024,
	month        = feb,
	publisher    = {arXiv},
	number       = {arXiv:2305.10790},
	doi          = {10.48550/arXiv.2305.10790},
	url          = {http://arxiv.org/abs/2305.10790},
	note         = {arXiv:2305.10790 [eess]},
	abstractnote = {The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.}
}
@inproceedings{Hao_Zhou_Liu_Li_Hu_Wang_Wei_2025,
	title        = {Boosting Large Language Model for Speech Synthesis: An Empirical Study},
	author       = {Hao, Hongkun and Zhou, Long and Liu, Shujie and Li, Jinyu and Hu, Shujie and Wang, Rui and Wei, Furu},
	year         = 2025,
	month        = apr,
	booktitle    = {ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {1–5},
	doi          = {10.1109/ICASSP49660.2025.10890588},
	issn         = {2379-190X},
	url          = {https://ieeexplore.ieee.org/abstract/document/10890588},
	abstractnote = {Large language models (LLMs) have made significant advancements in natural language processing and are concurrently extending the language ability to other modalities, such as speech and vision. Nevertheless, most of the previous work focuses on prompting LLMs with perception abilities like auditory comprehension, and the effective approach for augmenting LLMs with speech synthesis capabilities remains ambiguous. In this paper, we conduct a comprehensive empirical exploration of boosting LLMs with the ability to generate speech, by combining pre-trained LLM LLaMA/OPT and text-to-speech synthesis model VALL-E. We compare three integration methods between LLMs and speech synthesis models, including directly fine-tuned LLMs, superposed layers of LLMs and VALL-E, and coupled LLMs and VALL-E using LLMs as a powerful text encoder. Experimental results show that, using LoRA method to fine-tune LLMs directly to boost the speech synthesis capability does not work well, and superposed LLMs and VALL-E can improve the quality of generated speech both in speaker similarity and word error rate (WER). Among these three methods, coupled methods leveraging LLMs as the text encoder can achieve the best performance, making it outperform original speech synthesis models with a consistently better speaker similarity and a significant (10.9%) WER reduction.}
}
@article{Huang_Li_Yang_Shi_Chang_Ye_Wu_Hong_Huang_Liu_etal_2023,
	title        = {AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head},
	author       = {Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and Ren, Yi and Zhao, Zhou and Watanabe, Shinji},
	year         = 2023,
	month        = apr,
	publisher    = {arXiv},
	number       = {arXiv:2304.12995},
	doi          = {10.48550/arXiv.2304.12995},
	url          = {http://arxiv.org/abs/2304.12995},
	note         = {arXiv:2304.12995 [cs]},
	abstractnote = {Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at url{https://github.com/AIGC-Audio/AudioGPT}.}
}
@article{Kakouros_imko_Vainio_Suni_2023,
	title        = {Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody},
	author       = {Kakouros, Sofoklis and Šimko, Juraj and Vainio, Martti and Suni, Antti},
	year         = 2023,
	month        = jun,
	publisher    = {arXiv},
	number       = {arXiv:2306.09814},
	doi          = {10.48550/arXiv.2306.09814},
	url          = {http://arxiv.org/abs/2306.09814},
	note         = {arXiv:2306.09814 [eess]},
	abstractnote = {This paper investigates the use of word surprisal, a measure of the predictability of a word in a given context, as a feature to aid speech synthesis prosody. We explore how word surprisal extracted from large language models (LLMs) correlates with word prominence, a signal-based measure of the salience of a word in a given discourse. We also examine how context length and LLM size affect the results, and how a speech synthesizer conditioned with surprisal values compares with a baseline system. To evaluate these factors, we conducted experiments using a large corpus of English text and LLMs of varying sizes. Our results show that word surprisal and word prominence are moderately correlated, suggesting that they capture related but distinct aspects of language use. We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner. We demonstrate that, in line with these findings, a speech synthesizer conditioned with surprisal values provides a minimal improvement over the baseline with the results suggesting a limited effect of using surprisal values for eliciting appropriate prominence patterns.}
}
@article{Li_Chen_Yan_Shen_Xu_Wu_Zhang_Zhou_Chen_Cheng_etal_2023,
	title        = {ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models},
	author       = {Li, Chenliang and Chen, Hehong and Yan, Ming and Shen, Weizhou and Xu, Haiyang and Wu, Zhikai and Zhang, Zhicheng and Zhou, Wenmeng and Chen, Yingda and Cheng, Chen and Shi, Hongzhu and Zhang, Ji and Huang, Fei and Zhou, Jingren},
	year         = 2023,
	month        = sep,
	publisher    = {arXiv},
	number       = {arXiv:2309.00986},
	doi          = {10.48550/arXiv.2309.00986},
	url          = {http://arxiv.org/abs/2309.00986},
	note         = {arXiv:2309.00986 [cs]},
	abstractnote = {Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent libraryfootnote{https://github.com/modelscope/modelscope-agent} and online demofootnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.}
}
@inproceedings{Liu_Hussain_Sun_Shan_2024,
	title        = {Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning},
	author       = {Liu, Shansong and Hussain, Atin Sakkeer and Sun, Chenshuo and Shan, Ying},
	year         = 2024,
	month        = apr,
	booktitle    = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {286–290},
	doi          = {10.1109/ICASSP48485.2024.10447027},
	issn         = {2379-190X},
	url          = {https://ieeexplore.ieee.org/abstract/document/10447027},
	abstractnote = {Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.}
}
@article{Liu_Hussain_Wu_Sun_Shan_2024,
	title        = {M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models},
	author       = {Liu, Shansong and Hussain, Atin Sakkeer and Wu, Qilong and Sun, Chenshuo and Shan, Ying},
	year         = 2024,
	month        = dec,
	publisher    = {arXiv},
	number       = {arXiv:2311.11255},
	doi          = {10.48550/arXiv.2311.11255},
	url          = {http://arxiv.org/abs/2311.11255},
	note         = {arXiv:2311.11255 [cs]},
	abstractnote = {The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as text, speech, images, videos, etc. They also utilize LLMs to understand human intention and generate desired outputs like images, videos, and music. However, research that combines both understanding and generation using LLMs is still limited and in its nascent stage. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M$^{2}$UGen) framework that integrates LLM’s abilities to comprehend and generate music for different modalities. The M$^{2}$UGen framework is purpose-built to unlock creative potential from diverse sources of inspiration, encompassing music, image, and video through the use of pretrained MERT, ViT, and ViViT models, respectively. To enable music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging multi-modal understanding and music generation is accomplished through the integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA model to generate extensive datasets that support text/image/video-to-music generation, facilitating the training of our M$^{2}$UGen framework. We conduct a thorough evaluation of our proposed framework. The experimental results demonstrate that our model achieves or surpasses the performance of the current state-of-the-art models.}
}
@article{Liu_Zhu_Liu_Yuan_Huang_Cui_Liang_Cao_Kong_Plumbley_etal_2025,
	title        = {WavJourney: Compositional Audio Creation With Large Language Models},
	author       = {Liu, Xubo and Zhu, Zhongkai and Liu, Haohe and Yuan, Yi and Huang, Qiushi and Cui, Meng and Liang, Jinhua and Cao, Yin and Kong, Qiuqiang and Plumbley, Mark D. and Wang, Wenwu},
	year         = 2025,
	journal      = {IEEE Transactions on Audio, Speech and Language Processing},
	volume       = 33,
	pages        = {2830–2844},
	doi          = {10.1109/TASLPRO.2025.3574867},
	issn         = {2998-4173},
	abstractnote = {Despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. In a real-world scenario, however, we often need to generate audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation. WavJourney allows users to create storytelling audio content with diverse audio elements, simply based on textual descriptions. Specifically, given a text instruction, WavJourney first prompts LLMs to generate an audio script that serves as a structured semantic representation of audio elements. The audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. The computer program is then executed to obtain a compositional and interpretable solution for audio creation. Experimental results suggest that WavJourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-to-audio generation benchmarks. Additionally, we introduce a new multi-genre story benchmark. Subjective evaluations demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text. We further demonstrate that WavJourney can facilitate human-machine co-creation in multi-round dialogues.}
}
@inproceedings{Liu_Lai_Gao_Cui_Li_Zhu_Lu_Chen_Qiao_Dai_etal_2025,
	title        = {ControlLLM: Augment Language Models with Tools by Searching on Graphs},
	author       = {Liu, Zhaoyang and Lai, Zeqiang and Gao, Zhangwei and Cui, Erfei and Li, Ziheng and Zhu, Xizhou and Lu, Lewei and Chen, Qifeng and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
	year         = 2025,
	booktitle    = {Computer Vision – ECCV 2024},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {89–105},
	doi          = {10.1007/978-3-031-73254-6_6},
	isbn         = {978-3-031-73254-6},
	abstractnote = {We present ControlLLM, a novel framework that enables large language models (LLMs) to utilize multi-modal tools for solving complex real-world tasks. Despite the remarkable performance of LLMs, they still struggle with tool invocation due to ambiguous user prompts, inaccurate tool selection and mismatched input arguments. To overcome these challenges, our framework comprises three key components: (1) a task decomposer that breaks down a complex task into clear subtasks with well-defined inputs and outputs; (2) a Thoughts-on-Graph (ToG) paradigm that searches the optimal solution path on a pre-built tool graph, which specifies the parameter and dependency relations among different tools; and (3) an execution engine with a rich toolbox that interprets the solution path and runs the tools efficiently on different computational devices. We evaluate our framework on diverse tasks involving image, audio, and video processing, demonstrating its superior accuracy, efficiency, and versatility compared to existing methods. The code is available at https://github.com/OpenGVLab/ControlLLM.},
	editor       = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	language     = {en}
}
@inproceedings{Lu_Clark_Lee_Zhang_Khosla_Marten_Hoiem_Kembhavi_2024,
	title        = {Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action},
	author       = {Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha},
	year         = 2024,
	pages        = {26439–26455},
	url          = {https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.html},
	language     = {en}
}
@article{Rubenstein_Asawaroengchai_Nguyen_Bapna_Borsos_Quitry_Chen_Badawy_Han_Kharitonov_etal_2023,
	title        = {AudioPaLM: A Large Language Model That Can Speak and Listen},
	author       = {Rubenstein, Paul K. and Asawaroengchai, Chulayuth and Nguyen, Duc Dung and Bapna, Ankur and Borsos, Zalán and Quitry, Félix de Chaumont and Chen, Peter and Badawy, Dalia El and Han, Wei and Kharitonov, Eugene and Muckenhirn, Hannah and Padfield, Dirk and Qin, James and Rozenberg, Danny and Sainath, Tara and Schalkwyk, Johan and Sharifi, Matt and Ramanovich, Michelle Tadmor and Tagliasacchi, Marco and Tudor, Alexandru and Velimirović, Mihajlo and Vincent, Damien and Yu, Jiahui and Wang, Yongqiang and Zayats, Vicky and Zeghidour, Neil and Zhang, Yu and Zhang, Zhishuai and Zilka, Lukas and Frank, Christian},
	year         = 2023,
	month        = jun,
	publisher    = {arXiv},
	number       = {arXiv:2306.12925},
	doi          = {10.48550/arXiv.2306.12925},
	url          = {http://arxiv.org/abs/2306.12925},
	note         = {arXiv:2306.12925 [cs]},
	abstractnote = {We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples}
}
@article{Vyas_Shi_Le_Tjandra_Wu_Guo_Zhang_Zhang_Adkins_Ngan_etal_2023,
	title        = {Audiobox: Unified Audio Generation with Natural Language Prompts},
	author       = {Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and Wang, Jeff and Cruz, Ivan and Akula, Bapi and Akinyemi, Akinniyi and Ellis, Brian and Moritz, Rashel and Yungster, Yael and Rakotoarison, Alice and Tan, Liang and Summers, Chris and Wood, Carleigh and Lane, Joshua and Williamson, Mary and Hsu, Wei-Ning},
	year         = 2023,
	month        = dec,
	publisher    = {arXiv},
	number       = {arXiv:2312.15821},
	doi          = {10.48550/arXiv.2312.15821},
	url          = {http://arxiv.org/abs/2312.15821},
	note         = {arXiv:2312.15821 [cs]},
	abstractnote = {Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like “a person speaking” and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/}
}
@article{Wang_Chen_Wu_Zhang_Zhou_Liu_Chen_Liu_Wang_Li_etal_2023,
	title        = {Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers},
	author       = {Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and He, Lei and Zhao, Sheng and Wei, Furu},
	year         = 2023,
	month        = jan,
	publisher    = {arXiv},
	number       = {arXiv:2301.02111},
	doi          = {10.48550/arXiv.2301.02111},
	url          = {http://arxiv.org/abs/2301.02111},
	note         = {arXiv:2301.02111 [cs]},
	abstractnote = {We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.}
}
@inproceedings{Wang_Yang_Wu_Zhang_2024,
	title        = {Can Whisper Perform Speech-Based In-Context Learning?},
	author       = {Wang, Siyin and Yang, Chao-Han and Wu, Ji and Zhang, Chao},
	year         = 2024,
	month        = apr,
	booktitle    = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {13421–13425},
	doi          = {10.1109/ICASSP48485.2024.10446502},
	issn         = {2379-190X},
	url          = {https://ieeexplore.ieee.org/abstract/document/10446502},
	abstractnote = {This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to shed light on SICL’s adaptability to phonological variances and dialect-specific lexical nuances.}
}
@inproceedings{Wu_Gaur_Chen_Zhou_Zhu_Wang_Li_Liu_Ren_Liu_etal_2023,
	title        = {On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration},
	author       = {Wu, Jian and Gaur, Yashesh and Chen, Zhuo and Zhou, Long and Zhu, Yimeng and Wang, Tianrui and Li, Jinyu and Liu, Shujie and Ren, Bo and Liu, Linquan and Wu, Yu},
	year         = 2023,
	month        = dec,
	booktitle    = {2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
	pages        = {1–8},
	doi          = {10.1109/ASRU57964.2023.10389705},
	url          = {https://ieeexplore.ieee.org/abstract/document/10389705},
	abstractnote = {Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The “decoder-only“ architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.}
}
@article{Wu_Donahue_Watanabe_Bryan_2024,
	title        = {Music ControlNet: Multiple Time-Varying Controls for Music Generation},
	author       = {Wu, Shih-Lun and Donahue, Chris and Watanabe, Shinji and Bryan, Nicholas J.},
	year         = 2024,
	journal      = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	volume       = 32,
	pages        = {2692–2703},
	doi          = {10.1109/TASLP.2024.3399026},
	issn         = {2329-9304},
	abstractnote = {Text-to-music generation models are now capable of generating high-quality music audio in broad styles. However, text control is primarily suitable for the manipulation of global musical attributes like genre, mood, and tempo, and is less suitable for precise control over time-varying attributes such as the positions of beats in time or the changing dynamics of the music. We propose Music ControlNet, a diffusion-based music generation model that offers multiple precise, time-varying controls over generated audio. To imbue text-to-music models with time-varying control, we propose an approach analogous to pixel-wise control of the image-domain ControlNet method. Specifically, we extract controls from training audio yielding paired data, and fine-tune a diffusion-based conditional generative model over audio spectrograms given melody, dynamics, and rhythm controls. While the image-domain Uni-ControlNet method already allows generation with any subset of controls, we devise a new masking strategy to allow creators to input controls that are only partially specified in time. We evaluate both on controls extracted from audio and controls we expect creators to provide, demonstrating that we can generate realistic music that corresponds to control inputs in both settings. While few comparable music generation models exist, we benchmark against MusicGen, a recent model that accepts text and melody input, and show that our model generates music that is 49% more faithful to input melodies despite having 35x fewer parameters, training on 11x less data, and enabling two additional forms of time-varying control. Sound examples can be found at https://musiccontrolnet.github.io/web/.}
}
@article{Yang_Tian_Tan_Huang_Liu_Chang_Shi_Zhao_Bian_Zhao_etal_2024,
	title        = {UniAudio: An Audio Foundation Model Toward Universal Audio Generation},
	author       = {Yang, Dongchao and Tian, Jinchuan and Tan, Xu and Huang, Rongjie and Liu, Songxiang and Chang, Xuankai and Shi, Jiatong and Zhao, Sheng and Bian, Jiang and Zhao, Zhou and Wu, Xixin and Meng, Helen},
	year         = 2024,
	month        = dec,
	publisher    = {arXiv},
	number       = {arXiv:2310.00704},
	doi          = {10.48550/arXiv.2310.00704},
	url          = {http://arxiv.org/abs/2310.00704},
	note         = {arXiv:2310.00704 [cs]},
	abstractnote = {Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio}
}
@article{Yu_Song_Lu_He_Tan_Ye_Zhang_Bian_2023,
	title        = {MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models},
	author       = {Yu, Dingyao and Song, Kaitao and Lu, Peiling and He, Tianyu and Tan, Xu and Ye, Wei and Zhang, Shikun and Bian, Jiang},
	year         = 2023,
	month        = oct,
	publisher    = {arXiv},
	number       = {arXiv:2310.11954},
	doi          = {10.48550/arXiv.2310.11954},
	url          = {http://arxiv.org/abs/2310.11954},
	note         = {arXiv:2310.11954 [cs]},
	abstractnote = {AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience.}
}
@inproceedings{Yu_Tang_Sun_Chen_Tan_Li_Lu_Ma_Zhang_2024,
	title        = {Connecting Speech Encoder and Large Language Model for ASR},
	author       = {Yu, Wenyi and Tang, Changli and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
	year         = 2024,
	month        = apr,
	booktitle    = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {12637–12641},
	doi          = {10.1109/ICASSP48485.2024.10445874},
	issn         = {2379-190X},
	url          = {https://ieeexplore.ieee.org/abstract/document/10445874},
	abstractnote = {The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.}
}
@article{Yuan_Lin_Wang_Tian_Wu_Shen_Zhang_Wu_Liu_Zhou_etal_2024,
	title        = {ChatMusician: Understanding and Generating Music Intrinsically with LLM},
	author       = {Yuan, Ruibin and Lin, Hanfeng and Wang, Yi and Tian, Zeyue and Wu, Shangda and Shen, Tianhao and Zhang, Ge and Wu, Yuhang and Liu, Cong and Zhou, Ziya and Ma, Ziyang and Xue, Liumeng and Wang, Ziyu and Liu, Qin and Zheng, Tianyu and Li, Yizhi and Ma, Yinghao and Liang, Yiming and Chi, Xiaowei and Liu, Ruibo and Wang, Zili and Li, Pengfei and Wu, Jingcheng and Lin, Chenghua and Liu, Qifeng and Jiang, Tao and Huang, Wenhao and Chen, Wenhu and Benetos, Emmanouil and Fu, Jie and Xia, Gus and Dannenberg, Roger and Xue, Wei and Kang, Shiyin and Guo, Yike},
	year         = 2024,
	month        = feb,
	publisher    = {arXiv},
	number       = {arXiv:2402.16153},
	doi          = {10.48550/arXiv.2402.16153},
	url          = {http://arxiv.org/abs/2402.16153},
	note         = {arXiv:2402.16153 [cs]},
	abstractnote = {While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity’s creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.}
}
@article{Zhang_Li_Zhang_Zhan_Wang_Zhou_Qiu_2023,
	title        = {SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities},
	author       = {Zhang, Dong and Li, Shimin and Zhang, Xin and Zhan, Jun and Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	number       = {arXiv:2305.11000},
	doi          = {10.48550/arXiv.2305.11000},
	url          = {http://arxiv.org/abs/2305.11000},
	note         = {arXiv:2305.11000 [cs]},
	abstractnote = {Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.}
}
@article{Zhang_Maezawa_Xia_Yamamoto_Dixon_2024,
	title        = {Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing},
	author       = {Zhang, Yixiao and Maezawa, Akira and Xia, Gus and Yamamoto, Kazuhiko and Dixon, Simon},
	year         = 2024,
	month        = aug,
	publisher    = {arXiv},
	number       = {arXiv:2310.12404},
	doi          = {10.48550/arXiv.2310.12404},
	url          = {http://arxiv.org/abs/2310.12404},
	note         = {arXiv:2310.12404 [cs]},
	abstractnote = {Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user’s requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.}
}
@article{Zhuo_Yuan_Pan_Ma_LI_Zhang_Liu_Dannenberg_Fu_Lin_etal_2024,
	title        = {LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT},
	author       = {Zhuo, Le and Yuan, Ruibin and Pan, Jiahao and Ma, Yinghao and LI, Yizhi and Zhang, Ge and Liu, Si and Dannenberg, Roger and Fu, Jie and Lin, Chenghua and Benetos, Emmanouil and Xue, Wei and Guo, Yike},
	year         = 2024,
	month        = jul,
	publisher    = {arXiv},
	number       = {arXiv:2306.17103},
	doi          = {10.48550/arXiv.2306.17103},
	url          = {http://arxiv.org/abs/2306.17103},
	note         = {arXiv:2306.17103 [cs]},
	abstractnote = {We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today’s most performant chat-based large language model. In the proposed method, Whisper functions as the “ear” by transcribing the audio, while GPT-4 serves as the “brain,” acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.}
}
@article{Huang_Ren_Huang_Yang_Ye_Zhang_Liu_Yin_Ma_Zhao_2023,
	title        = {Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation},
	author       = {Huang, Jiawei and Ren, Yi and Huang, Rongjie and Yang, Dongchao and Ye, Zhenhui and Zhang, Chen and Liu, Jinglin and Yin, Xiang and Ma, Zejun and Zhao, Zhou},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	number       = {arXiv:2305.18474},
	doi          = {10.48550/arXiv.2305.18474},
	url          = {http://arxiv.org/abs/2305.18474},
	note         = {arXiv:2305.18474 [cs]},
	abstractnote = {Large diffusion models have been successful in text-to-audio (T2A) synthesis tasks, but they often suffer from common issues such as semantic misalignment and poor temporal consistency due to limited natural language understanding and data scarcity. Additionally, 2D spatial structures widely used in T2A works lead to unsatisfactory audio quality when generating variable-length audio samples since they do not adequately prioritize temporal information. To address these challenges, we propose Make-an-Audio 2, a latent diffusion-based T2A method that builds on the success of Make-an-Audio. Our approach includes several techniques to improve semantic alignment and temporal consistency: Firstly, we use pre-trained large language models (LLMs) to parse the text into structured <event & order> pairs for better temporal information capture. We also introduce another structured-text encoder to aid in learning semantic alignment during the diffusion denoising process. To improve the performance of variable length generation and enhance the temporal information extraction, we design a feed-forward Transformer-based diffusion denoiser. Finally, we use LLMs to augment and transform a large amount of audio-label data into audio-text datasets to alleviate the problem of scarcity of temporal data. Extensive experiments show that our method outperforms baseline models in both objective and subjective metrics, and achieves significant gains in temporal information understanding, semantic consistency, and sound quality.}
}
