\subsection{Training Strategy}
\label{sec:training-strategy}

We employ supervised fine-tuning (SFT) to train our model on the synthesized instruction-response pairs.
Our model architecture combines the frozen Qwen2-7B-Instruct backbone with 4-bit quantization, the frozen MERT-v1-330M audio encoder, the trainable MLP projection network, and Low-Rank Adaptation (LoRA) adapters on all linear layers of the language model.

The projection network consists of four hidden layers using GELU activation functions, layer normalization, and dropout regularization.
LoRA adapters are applied to the attention projection layers and the feed-forward network layers.
This configuration allows the language model to adapt its internal representations to better process the audio-conditioned inputs while maintaining parameter efficiency.
Each flawed mix is also augmented during training by applying a random gain adjustment within a predefined range of $-3$ to $+3$ dB and adding a DC offset within a range of $\pm 0.005$ and random noise within a predefined range of $-30$ to $60$ dB to the audio randomly.
This prevents the model from overfitting to the specific audio features of the flawed mixes.

The model is implemented using PyTorch~\cite{pytorch} with the HuggingFace Transformers library~\cite{wolf-etal-2020-transformers} and the Parameter-Efficient Fine-Tuning (PEFT) framework~\cite{peft}.
To enable efficient training of large language models, we employ 4-bit NormalFloat (NF4) quantization~\cite{dettmers2023qlora} with double quantization and bfloat16 compute type through the bitsandbytes library~\cite{bitsandbytes}.
The audio encoder remains frozen throughout training, while the projection layer and LoRA adapters are trained end-to-end.

The model is trained using the AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $1 \times 10^{-4}$.
We employ a cosine learning rate schedule with a warmup period covering 1\% of the total training steps.
Training is conducted with an effective batch size of 16, achieved through a per-device batch size of 2 and gradient accumulation over 8 steps.

The training objective combines the standard causal language modeling (CLM) loss with an auxiliary projection loss weighted at 0.05.
This auxiliary loss ensures strong gradient flow to the projection layer.



