Automatic mixing systems based on deep learning have achieved significant progress in producing professional-quality mixes, but they operate as ``black boxes'' that cannot explain their reasoning or provide interpretable feedback. This limitation hinders their adoption in educational and collaborative music production workflows. We present MixingBuddy, a multimodal large language model that addresses this gap by analyzing multitrack audio and generating structured textual feedback identifying mixing flaws and suggesting corrective gain adjustments. Our architecture employs MERT-v1-330M as an audio encoder, a trainable projection layer to align audio embeddings with the language model's input space, and Qwen2-7B-Instruct as the backbone, fine-tuned with parameter-efficient LoRA adapters. We focus on relative gain relationships among multitrack stems as a tractable starting point for investigating whether multimodal LLMs can learn to reason about mixing balance. On a test set of 25,650 samples, MixingBuddy achieves 49.85\% accuracy for the joint task of correctly identifying both the target stem and adjustment direction, with 56.20\% stem identification accuracy and 58.43\% direction prediction accuracy. When the model correctly identifies the stem and direction, it demonstrates strong magnitude prediction performance (76.06\% accuracy). The model shows particularly strong performance on extreme imbalances (78.8\% for very loud, 69.3\% for very quiet) and exhibits better performance on vocal stems compared to drums and bass, suggesting that semantic understanding provides advantages for certain instrument types. Our work demonstrates that multimodal LLMs can provide interpretable, structured feedback on mixing quality, bridging the gap between automated mixing systems and human understanding.