Automatic mixing systems based on deep learning have achieved significant progress in producing professional-quality mixes, but they operate as ``black boxes'' that cannot explain their reasoning.
We present MixingBuddy-gain, a multimodal large language model that addresses this gap by analyzing multitrack audio and generating structured textual feedback identifying mixing flaws and suggesting corrective gain adjustments.
Our architecture employs a pretrained audio encoder, a trainable projection layer to align audio embeddings with the language model's input space, and an instruction-tuned large language model as the backbone, fine-tuned with parameter-efficient LoRA adapters.
On a test set of 25,650 samples, MixingBuddy achieves 49.85\% accuracy for the joint task of correctly identifying both the target stem and adjustment direction.
Our work demonstrates that multimodal LLMs can provide interpretable, structured feedback on mixing quality, bridging the gap between automated mixing systems and human understanding.
Code and weights are available at \url{https://github.com/rp-bot/MixingBuddy-gain}.