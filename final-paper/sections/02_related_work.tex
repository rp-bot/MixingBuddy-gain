\section{Related Work}
\label{sec:related_work}

Early automatic mixing research centered on systems that captured expert knowledge through explicit mixing rules and heuristics.
\cite{Perez_Gonzalez_Reiss_2013} developed an autonomous mixing system based on knowledge engineering principles.
\cite{bocko2010automatic} employed probabilistic expert systems, a formal knowledge-based approach from early AI research, for automatic music production.
Subsequent work explored machine learning techniques for instrument-specific effects, such as \cite{Chourdakis_Reiss_2017}'s approach to intelligent artificial reverberation application.
\cite{Benito_Reiss_2017} presented intelligent multitrack reverberation based on hinge-loss Markov random fields, demonstrating the application of statistical modeling tools to mixing tasks.
\cite{Hilsamer_Herzog_2014} described a statistical approach to automated offline dynamic processing in the audio mastering process, further exemplifying the use of traditional machine learning methods.
\cite{Kolasinski_2008} introduced a framework that uses genetic optimization with timbral similarity measures.
While these methods provided interpretable control and domain-specific optimization, they were limited in their ability to generalize across diverse musical styles and lacked the flexibility to adapt to unseen mixing scenarios.

The advent of deep learning brought significant advances in automatic mixing, with models capable of learning complex mappings from raw audio to mixing parameters.
\cite{waveunet} introduced Wave-U-Net autoencoders for automatic mixing, demonstrating that end-to-end neural architectures could produce professional-quality mixes.
\cite{Steinmetz_Pons_Pascual_Serra_2020} further advanced the field with a differentiable mixing console incorporating neural audio effects, enabling gradient-based optimization of mixing parameters.
These deep learning approaches achieved notable success in terms of accuracy (matching professional mixes), controllability (allowing high-level parameter adjustment), and diversity (accommodating different genres and styles).
However, a critical limitation of these systems is their ``black box'' nature: while they can perform the mix, they cannot explain their reasoning or provide linguistic feedback about mixing decisions.

Recognizing the need to bridge the semantic gap between audio processing and human understanding, researchers began exploring word-embedding approaches that link natural language descriptions to audio effect parameters.
\cite{Venkatesh_Moffat_Miranda_2022} demonstrated word embeddings for automatic equalization in audio mixing, while \cite{Chu_OReilly_Barnett_Pardo_2025} developed Text2FX, which harnesses CLAP embeddings for text-guided audio effects.
Early semantic mixing approaches \cite{semantic-mixing} laid the groundwork for understanding how high-level, semantic knowledge could inform mixing decisions.
These methods represented initial attempts to make mixing systems more interpretable and user-friendly by connecting linguistic descriptions to audio processing parameters.

Building on language-audio integration, recent work has explored prompt-driven interfaces that map natural language instructions directly to mixing tasks.
\cite{Doh_Koo_Martinez-Ramirez_Liao_Nam_Mitsufuji_2025} investigated whether large language models can predict audio effects parameters from natural language, while \cite{Melechovsky_Mehrish_Herremans_2025} developed SonicMaster, a controllable all-in-one music restoration and mastering system.
\cite{Clemens_Marasovic_2025} introduced MixAssist, an audio-language dataset for co-creative AI assistance in music mixing, demonstrating the potential for collaborative human-AI mixing workflows.
These approaches represent an evolution toward more natural interaction paradigms, where users can express mixing intentions in natural language rather than manipulating low-level parameters.

Multimodal audio-language models have emerged as a powerful paradigm for combining audio understanding with language reasoning capabilities.
One architectural approach, direct tokenization (also known as the unified approach), converts raw audio into discrete tokens via audio codecs and extends the LLM vocabulary to include these audio tokens.
This enables the language model to process audio and text within a unified framework.
Key works in this direction include \cite{Rubenstein_Asawaroengchai_Nguyen_Bapna_Borsos_Quitry_Chen_Badawy_Han_Kharitonov_etal_2023}'s AudioPaLM, \cite{Du_Wang_Chen_Chu_Gao_Li_Hu_Zhou_Xu_Ma_etal_2024}'s LauraGPT, and \cite{Zhang_Li_Zhang_Zhan_Wang_Zhou_Qiu_2023}'s SpeechGPT.
The unified approach offers the advantage of treating audio and text as first-class citizens within the same model architecture, potentially enabling more seamless cross-modal reasoning.

An alternative architectural paradigm, the cascade (or feature extraction) approach, uses audio-specific encoders and decoders with the LLM serving as a central backbone.
In this framework, audio is first encoded into feature representations that are then processed by the language model, which can generate text responses or guide audio generation.
Examples include \cite{Liu_Hussain_Wu_Sun_Shan_2024}'s M$^{2}$UGen and \cite{Gong_Luo_Liu_Karlinsky_Glass_2024}'s Listen, Think, and Understand (LTU).
The cascade approach allows for specialized audio processing while leveraging the reasoning capabilities of large language models, making it particularly relevant for tasks requiring both audio understanding and linguistic explanation, such as mix critique and advice.

Multimodal audio-language models show promise for explainable audio processing, but their application to mix critique and advice remains largely unexplored.
Our work addresses this gap by leveraging multimodal audio-language models to provide linguistic feedback and reasoning about mixes.
This positions our work as a step toward explainable, co-creative mixing systems that bridge the gap between automated processing and human understanding.
