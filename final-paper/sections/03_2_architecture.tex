\subsection{Architecture}
\label{sec:architecture}

Our architecture implements a multimodal framework that processes 10-second audio segments paired with textual instructions to generate accurate text feedback addressing gain balancing anomalies.
Following the audio prefixing approach demonstrated in multimodal language models~\cite{Gong_Luo_Liu_Karlinsky_Glass_2024}, we concatenate audio embeddings with text embeddings before passing them to the large language model.
This design requires training an audio projection layer to align the encoder output embeddings (1024 dimensions for MERT~\cite{MERT-v1-330M}) with the LLM input embedding space (3584 dimensions for Qwen2-7B-Instruct)~\cite{Gong_Luo_Liu_Karlinsky_Glass_2024}.
While alternative modality alignment techniques exist~\cite{Liu_Hussain_Wu_Sun_Shan_2024}, audio prefixing provides an effective starting point for our application.
Figure~\ref{fig:architecture} illustrates the complete architecture.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figures/architecture}
\caption{Architecture overview showing the multimodal framework with audio encoder, projection layer, and language model components with LoRA adapters.}
\label{fig:architecture}
\end{figure*}


\subsubsection{Audio Encoder}
\label{sec:audio-encoder}

We employ the MERT-v1-330M encoder~\cite{MERT-v1-330M} as our audio feature extractor.
The encoder produces 25 transformer layers of representations, from which we extract a weighted average using learnable layer weights.
This weighted aggregation allows the model to adaptively emphasize different hierarchical levels of audio representation during training.
The encoder outputs 1024-dimensional embeddings at each time step.

\subsubsection{Audio Projection}
\label{sec:audio-projection}

To bridge the gap between audio encoder outputs and LLM input embeddings, we employ a multi-layer perceptron (MLP) projection network that maps 1024-dimensional audio embeddings to embeddings compatible with the LLM's input space.
Pilot experiments demonstrated that increasing the MLP depth or incorporating transformer layers or other complex architectures did not yield performance improvements.
Consequently, we adopt a simple MLP architecture with four hidden layers with dimensions [2048, 4096, 4096, 2048], using GELU activation functions, layer normalization, and a dropout rate that begins at 0.3 and is reduced to 0.1 once training plateaus.
We do not employ residual connections in the projection network.
The projected audio embeddings are concatenated with text token embeddings and passed to the language model for processing.

\subsubsection{Language Model}
\label{sec:language-model}

We employ the Qwen2-7B-Instruct language model~\cite{Qwen2-7B-Instruct} as the backbone.
We train LoRA adapters on the language model to allow fine-tuning while keeping the base model frozen.
We apply LoRA to the attention projection layers (\texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{o\_proj}) and the feed-forward network layers (\texttt{gate\_proj}, \texttt{up\_proj}, \texttt{down\_proj}).
The LoRA adapters use a rank $r=16$, scaling factor $\alpha=32$, and dropout rate of 0.1.
This allows the language model to both pay attention to these new audio embeddings and generate the advice responses in our expected format.



% The language model is trained using the HuggingFace Transformers library~\cite{wolf-etal-2020-transformers} and the Parameter-Efficient Fine-Tuning (PEFT) framework~\cite{peft}.
% The language model is trained using the HuggingFace Transformers library~\cite{wolf-etal-2020-transformers} and the Parameter-Efficient Fine-Tuning (PEFT) framework~\cite{peft}.