\section{Conclusion and Future Work}
\label{sec:conclusion}

We presented MixingBuddy-gain, a multimodal large language model that provides structured mixing advice by analyzing flawed mixtures and generating textual feedback identifying problematic stems and suggesting gain adjustments.
We curated a dataset of approximately 100,000 instruction-response pairs derived from the MUSDB18HQ dataset, employing a synthetic flaw generation pipeline that introduces controlled gain imbalances across bass, drums, and vocal stems. 
Our methodology integrated a frozen MERT-v1-330M audio encoder with a Qwen2-7B-Instruct language model via a multi-layer perceptron projection network, utilizing audio prefixing to condition the LLM on spectral features. 
The model was trained using Supervised Fine-Tuning (SFT) with Low-Rank Adaptation (LoRA).

The model achieved promising accuracy on the joint task of correctly identifying both the target stem and adjustment direction, with balanced performance for stem identification and direction prediction.
When the model correctly identified the stem and direction, it demonstrated strong performance in magnitude prediction, indicating that the model could reliably determine appropriate adjustment ranges once the core problem was identified.
Performance analysis also revealed that the model excelled at identifying extreme imbalances but struggled with moderate imbalances and correctly identifying well-balanced mixes. 
This indicated that while the model was effective at detecting problems, it sometimes incorrectly flagged well-balanced mixes as problematic.
Direction prediction showed the highest false positive rate and the lowest F1 score, indicating that the model frequently predicted the wrong adjustment direction, which is concerning as incorrect direction predictions would worsen mix balance.

These flaws could be attributed to a limitation of the current architecture or the dataset containing inconsistent mixtures. 
Regarding the architecture, the gradients of the model vanish as it reaches the projection layer  (Figure~\ref{fig:architecture}). 
We used an auxiliary loss to enforce alignment between the model's predictions and the audio.
But this is merely a workaround and not a true solution.
Another potential problem is that currently each time step is projected to the LLM input embedding space. 
This may not be the most efficient as this creates a tensor of shape $\left(\text{batch\_size}, \text{time\_steps}, \text{embedding\_size}\right)$ which in our case is $\left(\text{batch\_size}, 750, 1024\right)$ before projection and $\left(\text{batch\_size}, 750, 3584\right)$ after projection.
This may confuse the model as it needs to attend to a lot of information. 

Future work could explore a multi step alignment process where a) Train an audio projection layer that is trained to be aligned with the text description of the audio in the LLM's embedding space. 
b) Use the trained projection layer and replace it in our pipeline (Figure~\ref{fig:architecture}) while keeping it frozen.
The projection layer can be further strengthened by training it with a contrastive loss objective. 

Supervised fine-tuning as shown in this work is not integrated into the huggingface transformers Param Efficient Fine-Tuning (PEFT) pipeline.
Hence, we propose it as a framework that can be used as an extension to the PEFT pipeline.
We have made the entire codebase available at \url{https://github.com/rp-bot/MixingBuddy-gain}.
We also provide the weights of the model at \url{insert link} for easy access.

