# @package _global_

# DPO Training - Experiment 25
# Using variations dataset with all modules LoRA (embed_tokens, lm_head, attention, MLP)
# Based on qwen2_7b_mert_musdb_expanded_all_modules model config

defaults:
  - data: 10_dpo_data_variations
  - model: qwen2_7b_mert_musdb_expanded_all_modules_dpo
  - training: 14_dpo_training_fixed
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - _self_

# DPO-specific data settings
data:
  limit: 10000  # Use full dataset

# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-milestone-0"
  output_dir: "outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}"

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false

hydra:
  run:
    dir: ${env.output_dir}

# Training resume settings - update checkpoint path as needed
training:
  resume:
    enabled: true
    weight_only: true  # Load weights but not optimizer state (required for DPO)
    # Update this to point to your SFT checkpoint (experiment 22 or 23)
    checkpoint_path: "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-musdb-expanded-all-modules-r16a32-musdb"

