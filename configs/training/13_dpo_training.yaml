# DPO Training Arguments
# Configuration for Direct Preference Optimization training

# Standard training arguments
training_args:
  output_dir: "./outputs/checkpoints/mixing_buddy_milestone_0"
  num_train_epochs: 3  # DPO typically needs fewer epochs than SFT
  per_device_train_batch_size: 1  # DPO requires 2x memory (chosen + rejected)
  gradient_accumulation_steps: 32  # or even 64  # Increased to maintain effective batch size
  lr_scheduler_type: "cosine"
  learning_rate:  2e-7  # Lower LR for DPO (typically 1/10 to 1/2 of SFT LR)
  weight_decay: 0.01
  warmup_ratio: 0  # Small warmup to stabilize initial training
  max_grad_norm: 1.0
  logging_steps: 5
  save_steps: 200
  logging_first_step: true
  logging_strategy: "steps"
  save_total_limit: 2
  save_strategy: "steps"
  eval_strategy: "no"  # Disable evaluation entirely
  load_best_model_at_end: false
  report_to: "none"
  run_name: null
  disable_tqdm: false

# DPO-specific parameters
dpo:
  beta: 0.5  # Temperature parameter (higher = stronger preference optimization) - Increased from 0.1 to overcome length bias
  loss_type: "sigmoid"  # Options: "sigmoid", "hinge", "ipo"
  label_smoothing: 0.0  # Label smoothing for DPO loss

# Mixed precision training
mixed_precision:
  enabled: true
  dtype: "bf16"

# Early stopping
early_stopping:
  enabled: false  # Disabled since evaluation is disabled
  patience: 5

# Resume settings
resume:
  enabled: false
  weight_only: false
  checkpoint_path: ""

