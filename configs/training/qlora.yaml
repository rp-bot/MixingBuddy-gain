# QLoRA training configuration
training_args:
  output_dir: "./outputs"
  num_train_epochs: 3 # Fewer epochs for QLoRA
  per_device_train_batch_size: 2 # Larger batch size with QLoRA
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4 # Smaller accumulation
  lr_scheduler_type: "cosine"
  learning_rate: 2e-4 # Higher LR for QLoRA
  weight_decay: 0.01
  warmup_ratio: 0.1
  logging_steps: 10
  eval_steps: 100
  save_steps: 100
  save_total_limit: 3
  save_strategy: "steps"
  eval_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: "wandb"
  run_name: null

# Mixed precision training
mixed_precision:
  enabled: true
  dtype: "bf16"

# Early stopping
early_stopping:
  enabled: true
  patience: 2 # Less patience for QLoRA
