# @package training

# Training arguments for stem classification and gain regression
# Improved configuration for better capacity utilization
training_args:
  output_dir: "./outputs/checkpoints/mixing_buddy_milestone_0"
  
  num_train_epochs: 30
  per_device_train_batch_size: 64  # Increased from 8 for V100 32GB
  per_device_eval_batch_size: 64   # Increased from 8 for V100 32GB
  gradient_accumulation_steps: 2   # Reduced from 4 (effective batch: 16*2=32, same as before)
  
  learning_rate: 1e-4  # Increased from 7.5e-5 to allow faster learning with more capacity
  weight_decay: 0.1     # Keep regularization strong
  lr_scheduler_type: "cosine"
  warmup_steps: 500
  max_grad_norm: 1.0
  
  logging_steps: 20
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2
  
  eval_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  logging_first_step: true
  logging_strategy: "steps"
  report_to: "none"
  disable_tqdm: true

# Loss weights for multi-task learning
classification_weight: 1.0
regression_weight: 0.15  # Increased from 0.1 to give regression task more weight

# Evaluation settings
evaluation:
  # Optional cap on number of validation samples for faster eval
  max_eval_samples: 10000

# Early stopping
early_stopping:
  enabled: true   # Enable early stopping to prevent overfitting
  patience: 5     # Wait 5 eval cycles (2500 steps) before stopping

# Resume settings
resume:
  enabled: false
  weight_only: true  # Use weight-only since we changed regularization (dropout, weight_decay, lr)
  checkpoint_path: null

