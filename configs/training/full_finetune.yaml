# Full fine-tuning configuration
training_args:
  output_dir: "./outputs"
  num_train_epochs: 2 # Fewer epochs for full fine-tuning
  per_device_train_batch_size: 1 # Smaller batch size
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16 # Larger accumulation
  lr_scheduler_type: "linear"
  learning_rate: 5e-5 # Lower LR for full fine-tuning
  weight_decay: 0.1 # Higher weight decay
  warmup_ratio: 0.05 # Less warmup
  logging_steps: 20
  eval_steps: 200
  save_steps: 200
  save_total_limit: 2
  save_strategy: "steps"
  eval_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: "wandb"
  run_name: null

# Mixed precision training
mixed_precision:
  enabled: true
  dtype: "fp16" # Use fp16 for full fine-tuning

# Early stopping
early_stopping:
  enabled: true
  patience: 1 # Very strict for full fine-tuning
