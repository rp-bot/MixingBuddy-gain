# @package training

# Training arguments
training_args:
  output_dir: "outputs/checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 10
  eval_steps: 100
  save_steps: 500
  save_total_limit: 3
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: "wandb"
  run_name: null # Will be set automatically

# Optimizer
optimizer:
  type: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8

# Scheduler
scheduler:
  type: "cosine"
  num_warmup_steps: null # Will be calculated from warmup_ratio

# Gradient clipping
gradient_clipping:
  max_grad_norm: 1.0

# Mixed precision
mixed_precision:
  enabled: true
  dtype: "bf16"

# Checkpointing
checkpointing:
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true

# Early stopping
early_stopping:
  enabled: true
  patience: 3
  threshold: 0.001
