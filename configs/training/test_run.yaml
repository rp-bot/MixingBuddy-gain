# @package training

# Test training configuration - minimal parameters for quick testing
training_args:
  output_dir: "outputs/test_run"
  num_train_epochs: 1 # Just 1 epoch for testing
  per_device_train_batch_size: 2 # Test larger batch size
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2 # Test gradient accumulation
  learning_rate: 5e-4 # Slightly higher for faster convergence
  weight_decay: 0.01
  warmup_ratio: 0.1 # 10% warmup
  lr_scheduler_type: "linear"
  logging_steps: 1 # Log every step for monitoring
  eval_steps: 5 # Evaluate every 5 steps
  save_steps: 10 # Save every 10 steps
  save_total_limit: 2 # Keep only 2 checkpoints
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: "none" # Disable wandb for test run
  run_name: "test-run-$(date +%Y%m%d-%H%M%S)"
  max_steps: 20 # Limit to 20 steps for quick test
  dataloader_drop_last: false # Don't drop last batch for small dataset

# Optimizer
optimizer:
  type: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8

# Scheduler
scheduler:
  type: "linear"
  num_warmup_steps: 2 # 2 warmup steps

# Gradient clipping
gradient_clipping:
  max_grad_norm: 1.0

# Mixed precision - disabled for test run to avoid issues
mixed_precision:
  enabled: false
  dtype: "fp32"

# Checkpointing
checkpointing:
  save_strategy: "steps"
  save_steps: 10
  save_total_limit: 2
  load_best_model_at_end: true

# Early stopping - disabled for test run
early_stopping:
  enabled: false
  patience: 3
  threshold: 0.001
