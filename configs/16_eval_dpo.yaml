# @package _global_

# Evaluation config for DPO-trained model (experiment 16)
# This evaluates the model trained with DPO (Direct Preference Optimization)
# Model architecture: MERT encoder + MLP projection + LoRA on output layers
# Trains only: LoRA adapters (o_proj, down_proj, lm_head)
# Audio encoder and projection are frozen
defaults:
  - data: 05_musdb_expanded  # Use regular test dataset (not DPO pairs)
  - model: qwen2_7b_mert_dpo
  - evaluation: evaluation
  - training: 13_dpo_training  # Include DPO training config for compatibility
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - plotting: plotting
  - _self_

# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-dpo"
  # Output directory will be determined by checkpoint run name

# Model checkpoint settings
# Set this to the path of your trained DPO model checkpoint
# Options:
# - null: Use base model (no trained weights)
# - "latest": Automatically use the latest checkpoint
# - "path/to/checkpoint": Use specific checkpoint
checkpoint_path: "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-dpo-r8a16-musdb/checkpoint-800"  # Update this to your DPO checkpoint path

# IMPORTANT: The evaluation output directory will be automatically determined
# from the checkpoint path. For example, if checkpoint is:
# "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-dpo-r8a16-musdb-XXXX-XXXX/checkpoint-XXX"
# Then evaluation will be saved to:
# "outputs/evaluation/qlora-qwen2-7b-mert-dpo-r8a16-musdb-XXXX-XXXX/predictions/"

# Predictions file path for metrics computation
# If not specified, the script will use the new evaluation structure
# predictions_path: "outputs/evaluation/qlora-qwen2-7b-mert-dpo-r8a16-musdb-XXXX-XXXX/predictions/predictions.jsonl"

# Plotting configuration
plotting:
  # These will be automatically set based on checkpoint path
  metrics_file: null  # Will be set to {output_dir}/predictions/metrics_results_detailed.json
  output_dir: null  # Will be set to {output_dir}/predictions/

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false

# Evaluation-specific overrides for DPO model
evaluation:
  # Use the trained projection weights (not random)
  use_random_projection: false  # Use the trained projection weights

  # Generation samples for evaluation
  num_generation_samples: 100  # Multiple samples for better statistical significance

  # Memory settings - DPO model uses same architecture as SFT
  memory:
    max_memory_usage_gb: 8.0  # QLoRA keeps memory usage reasonable