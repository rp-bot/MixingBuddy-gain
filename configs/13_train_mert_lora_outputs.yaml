# @package _global_

# Training config for Experiment 13: Frozen projection + strategic LoRA adapters
# Loads pre-trained projection and MERT weights from experiment 11 checkpoint-34000
# Trains only LoRA adapters on output layers (o_proj, down_proj, lm_head)

defaults:
  - data: 05_musdb_expanded
  - model: qwen2_7b_mert_lora_outputs
  - training: 11_training
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - _self_

# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-milestone-0"
  output_dir: "outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}"

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false

# Training-specific overrides
training:
  # Resume configuration to load pre-trained projection weights
  resume:
    enabled: false # Don't resume training, just load projection/encoder weights
    checkpoint_path: "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-lora-outputs-r8a16-musdb/checkpoint-2000" # Experiment 11 checkpoint

hydra:
  run:
    dir: ${env.output_dir}

