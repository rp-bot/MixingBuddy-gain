# @package _global_

# Training config with MERT encoder (top 3 layers unfrozen) + LoRA adapters
# Experiment 19: MERT top3 + multi-anchor MUSDB data + LoRA adapters
# Tests MERT audio encoder with top layers unfrozen and LoRA adapters on LLM

defaults:
  - data: 05_musdb_expanded
  - model: qwen2_7b_mert_top3_bottom2_musdb_expanded_lora
  - training: 11_training
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - _self_

# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-milestone-0"
  output_dir: "outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}"

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false

hydra:
  run:
    dir: ${env.output_dir}

training:
  training_args:
    num_train_epochs: 15  # Reduced from 20 since we're training LoRA adapters + projection + MERT layers
  resume:
    enabled: false
    weight_only: false  # Set to true for weight-only resume (loads weights but starts fresh training)
    checkpoint_path: null

