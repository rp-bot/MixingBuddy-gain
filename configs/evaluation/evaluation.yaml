# Evaluation-specific settings

# Evaluation batch size (usually smaller than training)
batch_size: 4

# Maximum number of samples to evaluate (null for all samples)
max_samples: null

# Whether to save model predictions
save_predictions: true

# Output directory for predictions
# Note: Output directory is now determined by checkpoint run name
# predictions_output_dir: "${env.output_dir}/predictions"

# Parameters for the generation script (08_generate_samples.py)
num_generation_samples: 100 # Set to null to generate for all samples
max_new_tokens: 1024

# generation:
#   max_new_tokens: ${evaluation.max_new_tokens}
#   do_sample: false        # Disable sampling
#   num_beams: 1           # Use greedy decoding
  # Allow deterministic decoding when desired
  # Example override: evaluation.generation.do_sample=false

# Evaluation metrics to compute
metrics:
  - "loss"
  - "perplexity"
  - "accuracy" # If applicable

# Checkpoint settings
# Note: Checkpoint path is now handled in the main evaluate.yaml config
# use_latest_checkpoint: true # Automatically use latest checkpoint if no specific path is set

# Memory management
memory:
  cleanup_before_eval: true
  max_memory_usage_gb: 8.0 # Maximum GPU memory usage

# Model loading settings
model_loading:
  load_audio_projection: true
  strict_loading: true # Allow missing keys
  map_location: "cuda" # "cpu", "cuda", or "auto"

# Ablation study: Use random projection weights instead of trained ones
# This tests whether the trained audio projection actually helps
use_random_projection: false

# Custom metrics configuration (for 09_compute_metrics.py)
custom_metrics:
  semantic_model: "sentence-transformers/all-mpnet-base-v2"
  classifier_model: "MoritzLaurer/deberta-v3-base-zeroshot-v1"
