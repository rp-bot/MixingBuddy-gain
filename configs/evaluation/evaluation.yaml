# Evaluation-specific settings

# Evaluation batch size (usually smaller than training)
batch_size: 4

# Maximum number of samples to evaluate (null for all samples)
max_samples: null

# Whether to save model predictions
save_predictions: true

# Output directory for predictions
predictions_output_dir: "${env.output_dir}/predictions"

# Parameters for the generation script (08_generate_samples.py)
num_generation_samples: 20 # Set to null to generate for all samples
max_new_tokens: 1024

# Evaluation metrics to compute
metrics:
  - "loss"
  - "perplexity"
  - "accuracy" # If applicable

# Checkpoint settings
use_latest_checkpoint: true # Automatically use latest checkpoint if no specific path is set

# Memory management
memory:
  cleanup_before_eval: true
  max_memory_usage_gb: 8.0 # Maximum GPU memory usage

# Model loading settings
model_loading:
  load_audio_projection: true
  strict_loading: false # Allow missing keys
  map_location: "cuda" # "cpu", "cuda", or "auto"

# Custom metrics configuration (for 09_compute_metrics.py)
custom_metrics:
  compute_semantic_similarity: true
  compute_label_metrics: true
  semantic_model: "sentence-transformers/all-mpnet-base-v2"
  extraction_model: "Qwen/Qwen2.5-1.5B-Instruct"
  extraction_device: "cuda" # "cuda" or "cpu"
