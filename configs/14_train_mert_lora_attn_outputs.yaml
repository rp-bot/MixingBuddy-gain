# @package _global_

# Training config - Experiment 14: LoRA on attention + output layers
# Builds on experiment 13 by adding attention layer adapters (q_proj, k_proj, v_proj)
# Hypothesis: Adding attention adapters will improve quiet detection by helping the model
# learn better audio-text alignment

defaults:
  - data: 05_musdb_expanded
  - model: qwen2_7b_mert_lora_attn_outputs
  - training: 12_training
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - _self_

# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-milestone-0"
  output_dir: "outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}"

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false


hydra:
  run:
    dir: ${env.output_dir}


