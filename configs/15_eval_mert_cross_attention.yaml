# @package _global_

# Evaluation config for MERT + cross-attention projection (experiment 15)
# This evaluates the model trained with MERT encoder, cross-attention projection, and unfrozen layer weights
# Trains only: cross-attention projection + 25 MERT layer weights
# LLM is completely frozen (no LoRA adapters)
defaults:
  - data: 05_musdb_expanded
  - model: qwen2_7b_mert_cross_attention
  - evaluation: evaluation
  - training: 12_training # Include training config for trainer compatibility
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - plotting: plotting
  - _self_

# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-milestone-0"
  # Output directory will be determined by checkpoint run name

# Model checkpoint settings
# Set this to the path of your trained MERT + cross-attention model checkpoint
# Options:
# - null: Use base model (no trained weights)
# - "latest": Automatically use the latest checkpoint
# - "path/to/checkpoint": Use specific checkpoint
checkpoint_path: "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-cross-attention-r16a32-musdb/checkpoint-1200" # Update this to the trained checkpoint path

# IMPORTANT: The evaluation output directory will be automatically determined
# from the checkpoint path. For example, if checkpoint is:
# "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-cross-attention-r16a32-musdb-XXXX-XXXX/checkpoint-XXX"
# Then evaluation will be saved to:
# "outputs/evaluation/qlora-qwen2-7b-mert-cross-attention-r16a32-musdb-XXXX-XXXX/predictions/"

# Predictions file path for metrics computation
# If not specified, the script will use the new evaluation structure
# predictions_path: "outputs/evaluation/qlora-qwen2-7b-mert-cross-attention-r16a32-musdb-XXXX-XXXX/predictions/predictions.jsonl"

# Plotting configuration
plotting:
  # These will be automatically set based on checkpoint path
  metrics_file: null # Will be set to {output_dir}/predictions/metrics_results_detailed.json
  output_dir: null # Will be set to {output_dir}/predictions/

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false

# Evaluation-specific overrides for MERT + cross-attention model
evaluation:
  # Use the trained cross-attention projection weights
  use_random_projection: false # Use the trained projection weights

  # Generation samples for evaluation
  num_generation_samples: 100 # Multiple samples for better statistical significance

  # Memory settings - MERT should be memory efficient
  memory:
    max_memory_usage_gb: 8.0 # MERT is efficient but still needs some headroom

