# @package _global_

# Evaluation config - Experiment 14: LoRA on attention + output layers
# Evaluates model trained with LoRA adapters on attention + output layers

defaults:
  - data: 05_musdb_expanded
  - model: qwen2_7b_mert_lora_attn_outputs
  - evaluation: evaluation
  - training: 12_training
  - experiment_tracking: wandb
  - experiment_naming: experiment_naming
  - _self_


# Environment settings
env:
  seed: 42
  project_name: "automatic-mixing-milestone-0"
  # Output directory will be determined by checkpoint run name

# Model checkpoint settings
# Set this to the path of your trained experiment 13 model checkpoint
# Options:
# - null: Use base model (no trained weights)
# - "latest": Automatically use the latest checkpoint
# - "path/to/checkpoint": Use specific checkpoint
checkpoint_path: "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-lora-attn-outputs-r8a16-musdb/checkpoint-4800" # Update this to the trained experiment 14 checkpoint path

# IMPORTANT: The evaluation output directory will be automatically determined
# from the checkpoint path. For example, if checkpoint is:
# "outputs/checkpoints/mixing_buddy_milestone_0/qlora-qwen2-7b-mert-lora-outputs-r16a32-musdb-XXXX-XXXX/checkpoint-XXX"
# Then evaluation will be saved to:
# "outputs/evaluation/qlora-qwen2-7b-mert-lora-outputs-r16a32-musdb-XXXX-XXXX/predictions/"

# Predictions file path for metrics computation
# If not specified, the script will use the new evaluation structure
# predictions_path: "outputs/evaluation/qlora-qwen2-7b-mert-lora-outputs-r16a32-musdb-XXXX-XXXX/predictions/predictions.jsonl"

# Plotting configuration
plotting:
  # These will be automatically set based on checkpoint path
  metrics_file: null # Will be set to {output_dir}/predictions/metrics_results_detailed.json
  output_dir: null # Will be set to {output_dir}/predictions/

# Experiment tracking settings
experiment_tracking:
  use_wandb: false
  use_mlflow: false

# Evaluation-specific overrides
evaluation:
  # Use the trained projection weights (frozen from exp 11) + trained LoRA adapters
  use_random_projection: false # Use the trained projection weights

  # Generation samples for evaluation
  num_generation_samples: 100 # Multiple samples for better statistical significance

  # Memory settings - MERT should be memory efficient
  memory:
    max_memory_usage_gb: 8.0 # MERT is efficient but still needs some headroom

# Training resume config (needed for projection/encoder weight loading during eval)
training:
  resume:
    enabled: false
    checkpoint_path: null # Will be set to the same as checkpoint_path during evaluation


