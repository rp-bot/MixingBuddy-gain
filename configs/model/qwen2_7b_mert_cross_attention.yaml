# @package model

# Model configuration - Experiment 15: Cross-attention projection + unfrozen layer weights
# Similar to experiment 11 but uses cross-attention projection instead of MLP
# Trains only: cross-attention projection + 25 MERT layer weights
# LLM is completely frozen (no LoRA adapters)
model_name: "Qwen/Qwen2-7B-Instruct"
config_name: "qwen2_7b_mert_cross_attention"
use_qlora: true

# LoRA configuration with NO target modules
# The LLM is completely frozen
lora:
  r: 16
  lora_alpha: 32
  target_modules: [] # Empty list = no LoRA adapters!
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# QLoRA quantization configuration
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# Audio encoder configuration - MERT-v1-330M
encoder:
  model_name: "m-a-p/MERT-v1-330M"
  freeze: true # Freeze encoder weights
  freeze_layer_weights: false # Unfreeze the 25 learnable layer weights (trainable)
  device: cuda # Will be set automatically based on LLM device
  input_sample_rate: 24000 # Our data is at 24kHz, MERT will resample to 24kHz

# Audio projection configuration - Cross-attention
# MERT uses learnable weighted average of 25 layers -> 1024 dimensional output
# Cross-attention projection: mix attends to anchor, then projects to LLM space
projection:
  type: "cross_attention"
  num_heads: 8 # Number of attention heads
  dropout: 0.1 # Dropout rate
  # Auxiliary loss is CRITICAL here since projection is the only trainable component
  use_auxiliary_loss: true
  auxiliary_loss_weight: 0.05 # Increased weight since it's more important

