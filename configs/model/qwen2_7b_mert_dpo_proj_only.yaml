# @package model

# Model configuration for DPO training
# Based on qwen2_7b_mert_lora_outputs but adapted for DPO
# Loads from SFT checkpoint and continues with preference optimization
model_name: "Qwen/Qwen2-7B-Instruct"
config_name: "qwen2_7b_mert_dpo_proj_only"
use_qlora: true

# LoRA configuration - same as SFT for compatibility
# Target output layers for efficient adaptation
lora:
  r: 8
  lora_alpha: 16
  target_modules: []
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# QLoRA quantization configuration
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# Audio encoder configuration - MERT-v1-330M
encoder:
  model_name: "m-a-p/MERT-v1-330M"
  freeze: true
  freeze_layer_weights: false  # Keep frozen from SFT
  device: cuda
  input_sample_rate: 24000

# Audio projection configuration
# Load from SFT checkpoint - keep frozen during DPO
projection:
  type: "mlp"
  hidden_dims: [2048, 4096, 4096, 2048]
  activation: "relu"
  dropout: 0.1
  use_layer_norm: true
  use_residual: false
  freeze_projection: false  # Keep projection frozen during DPO
  use_auxiliary_loss: true
  auxiliary_loss_weight: 0.01

