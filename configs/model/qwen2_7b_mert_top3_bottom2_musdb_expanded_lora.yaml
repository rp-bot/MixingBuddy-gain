# @package model

# Model configuration - WITH LoRA adapters, train audio projection + top 3 + bottom 2 MERT encoder layers + LoRA
# MERT provides 25 layers * 1024 features -> learnable weighted average -> 1024 dimensional output
# Progressive unfreezing strategy:
#   - Top 3 encoder layers (22, 23, 24) are unfrozen to learn high-level semantic features
#   - Bottom 2 encoder layers (0, 1) are unfrozen to adapt low-level feature extraction
#   - Middle layers (2-21) remain frozen to preserve pretrained representations
# LoRA adapters are enabled on LLM to allow fine-tuning while keeping base model frozen
model_name: "Qwen/Qwen2-7B-Instruct"
config_name: "qwen2_7b_mert_top3_bottom2_musdb_expanded_lora"
use_qlora: true

# LoRA configuration with extended target modules
# Enables LoRA adapters on attention and MLP layers
lora:
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# QLoRA quantization configuration
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# Audio encoder configuration - MERT-v1-330M with progressive unfreezing (top 3 + bottom 2 layers)
encoder:
  model_name: "m-a-p/MERT-v1-330M"
  freeze: true # Freeze base model, but selectively unfreeze top and bottom layers
  unfreeze_top_n_layers: 3 # Unfreeze layers 22, 23, 24 for high-level semantic learning
  unfreeze_bottom_n_layers: 2 # Unfreeze layers 0, 1 for low-level feature adaptation
  device: cuda # Will be set automatically based on LLM device
  freeze_layer_weights: false
  input_sample_rate: 24000 # Our data is at 24kHz, MERT will resample to 24kHz

# Audio projection configuration
# MERT uses learnable weighted average of 25 layers -> 1024 dimensional output
# Need to project to LLM hidden size (typically 3584 for Qwen2-7B)
# With smaller input (1024), we can use a deeper, more expressive projection
projection:
  type: "mlp"
  # Deep projection: 1024 -> 2048 -> 4096 -> 2048 -> llm_hidden_size
  # More capacity since we saved memory with weighted averaging
  hidden_dims: [2048, 4096, 4096, 2048]
  activation: "relu"
  dropout: 0.1
  use_layer_norm: true
  use_residual: false
  freeze_projection: false
  # Auxiliary loss is CRITICAL here since projection is trainable
  use_auxiliary_loss: true
  auxiliary_loss_weight: 0.05 # Increased weight since it's more important

