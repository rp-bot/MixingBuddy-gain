# @package model

# Model configuration - Experiment 14: Frozen projection + LoRA on attention + outputs
# Freezes MERT encoder, MERT layer_weights, and pre-trained MLP projection from exp 11
# Trains LoRA adapters on attention layers (q_proj, k_proj, v_proj) + output layers (o_proj, down_proj, lm_head)
# This should help the model learn better audio-text alignment for subtle features (quiet detection)
model_name: "Qwen/Qwen2-7B-Instruct"
config_name: "qwen2_7b_mert_lora_attn_outputs"
use_qlora: true

# LoRA configuration with attention + output target modules
# Attention layers help model learn WHERE to look in audio embeddings
# Output layers help model learn WHAT to generate
lora:
  r: 8
  lora_alpha: 16
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "down_proj", "lm_head"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# QLoRA quantization configuration
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# Audio encoder configuration - MERT-v1-330M
encoder:
  model_name: "m-a-p/MERT-v1-330M"
  freeze: true
  freeze_layer_weights: true # Freeze the 25 learnable layer weights
  device: cuda # Will be set automatically based on LLM device
  input_sample_rate: 24000 # Our data is at 24kHz, MERT will resample to 24kHz

# Audio projection configuration
# MERT uses learnable weighted average of 25 layers -> 1024 dimensional output
# Need to project to LLM hidden size (typically 3584 for Qwen2-7B)
# Using same architecture as experiment 11
projection:
  type: "mlp"
  # Deep projection: 1024 -> 2048 -> 4096 -> 4096 -> 2048 -> llm_hidden_size
  hidden_dims: [2048, 4096, 4096, 2048]
  activation: "relu"
  dropout: 0.1
  use_layer_norm: true
  use_residual: false
  # Projection will be frozen and loaded from exp 11 checkpoint
  freeze_projection: true # Freeze projection weights
  # Auxiliary loss less important since projection is frozen, but keep for compatibility
  use_auxiliary_loss: true
  auxiliary_loss_weight: 0.01 # Reduced since projection is frozen


