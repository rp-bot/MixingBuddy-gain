"""
Compute custom evaluation metrics on saved predictions.

This script loads predictions from a JSONL file (generated by 08_generate_samples.py)
and computes:
1. Semantic similarity between ground truth and generated responses
2. Label extraction and accuracy (stem name, direction, magnitude)
"""

import hydra
from omegaconf import DictConfig
import sys
from pathlib import Path
import json
import gc
import torch
from typing import List, Dict
from tqdm import tqdm
import numpy as np

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.evaluation.metrics import (  # noqa: E402
    SemanticSimilarityMetric,
    LabelExtractor,
    LabelChecker,
)


def load_predictions(predictions_path: Path) -> List[Dict]:
    """Load predictions from JSONL file."""
    print(f"Loading predictions from {predictions_path}")
    predictions = []
    with open(predictions_path, "r") as f:
        for line in f:
            predictions.append(json.loads(line.strip()))
    print(f"Loaded {len(predictions)} predictions")
    return predictions


def compute_semantic_similarity(
    predictions: List[Dict],
    model_name: str,
) -> Dict:
    """
    Compute semantic similarity between ground truth and generated responses.

    Args:
        predictions: List of prediction dictionaries
        model_name: Name of the sentence transformer model

    Returns:
        Dictionary with mean, std, and per-sample similarities
    """
    print("\n" + "=" * 50)
    print("COMPUTING SEMANTIC SIMILARITY")
    print("=" * 50)

    metric = SemanticSimilarityMetric(model_name=model_name)

    similarities = []
    for pred in tqdm(predictions, desc="Computing similarities"):
        similarity = metric.compute(pred["ground_truth"], pred["generated"])
        similarities.append(similarity)

    results = {
        "mean": float(np.mean(similarities)),
        "std": float(np.std(similarities)),
        "median": float(np.median(similarities)),
        "min": float(np.min(similarities)),
        "max": float(np.max(similarities)),
        "per_sample": similarities,
    }

    print(f"\nSemantic Similarity Results:")
    print(f"  Mean: {results['mean']:.4f}")
    print(f"  Std:  {results['std']:.4f}")
    print(f"  Median: {results['median']:.4f}")
    print(f"  Min: {results['min']:.4f}")
    print(f"  Max: {results['max']:.4f}")

    # Clean up
    del metric
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return results


def compute_label_metrics(
    predictions: List[Dict],
    extraction_model_name: str,
    extraction_device: str,
) -> Dict:
    """
    Extract labels and compute accuracy metrics.

    Args:
        predictions: List of prediction dictionaries
        extraction_model_name: Name of the LLM for extraction
        extraction_device: Device to run extraction model on

    Returns:
        Dictionary with extraction results and accuracy metrics
    """
    print("\n" + "=" * 50)
    print("COMPUTING LABEL METRICS")
    print("=" * 50)

    # Initialize extractors
    extractor = LabelExtractor(
        model_name=extraction_model_name,
        device=extraction_device,
    )
    checker = LabelChecker()

    # Extract labels from generated responses
    extracted_labels = []
    for pred in tqdm(predictions, desc="Extracting labels"):
        extracted = extractor.extract(pred["generated"])
        extracted_labels.append(extracted)

    # Clean up extractor to free memory
    del extractor
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # Prepare ground truth data
    ground_truth_stems = [pred["target_stem"] for pred in predictions]
    ground_truth_error_categories = [pred["error_category"] for pred in predictions]
    ground_truth_intended_gains = [pred["intended_gain_db"] for pred in predictions]

    # Compute accuracy
    accuracy = checker.compute_accuracy(
        extracted_labels=extracted_labels,
        ground_truth_stems=ground_truth_stems,
        ground_truth_error_categories=ground_truth_error_categories,
        ground_truth_intended_gains_db=ground_truth_intended_gains,
    )

    print(f"\nLabel Accuracy Results:")
    print(f"  Stem Name:  {accuracy['stem_name']:.4f}")
    print(f"  Direction:  {accuracy['direction']:.4f}")
    print(f"  Magnitude:  {accuracy['magnitude']:.4f}")
    print(f"  Overall:    {accuracy['overall']:.4f}")

    # Combine extracted labels with predictions for detailed output
    detailed_results = []
    for i, pred in enumerate(predictions):
        # Get expected magnitude range from prediction if available
        expected_min = pred.get("expected_magnitude_min_db")
        expected_max = pred.get("expected_magnitude_max_db")

        detailed_results.append(
            {
                "global_uid": pred["global_uid"],
                "ground_truth": {
                    "stem_name": ground_truth_stems[i],
                    "direction": checker._get_ground_truth_direction(
                        ground_truth_error_categories[i]
                    ),
                    "error_category": ground_truth_error_categories[i],
                    "intended_gain_db": ground_truth_intended_gains[i],
                    "expected_magnitude_min_db": expected_min,
                    "expected_magnitude_max_db": expected_max,
                },
                "extracted": extracted_labels[i],
                "correct": {
                    "stem_name": checker.check_stem_name(
                        extracted_labels[i], ground_truth_stems[i]
                    ),
                    "direction": checker.check_direction(
                        extracted_labels[i], ground_truth_error_categories[i]
                    ),
                    "magnitude": checker.check_magnitude(
                        extracted_labels[i],
                        ground_truth_error_categories[i],
                        ground_truth_intended_gains[i],
                    ),
                },
            }
        )

    return {
        "accuracy": accuracy,
        "detailed_results": detailed_results,
    }


@hydra.main(config_path="../configs", config_name="evaluate", version_base=None)
def main(cfg: DictConfig):
    """
    Main metrics computation function.
    """
    print("=" * 50)
    print("CUSTOM METRICS COMPUTATION")
    print("=" * 50)

    # Get predictions path
    if hasattr(cfg, "predictions_path") and cfg.predictions_path:
        predictions_path = Path(cfg.predictions_path)
    else:
        # Default: use the predictions from the current output directory
        predictions_path = (
            Path(cfg.env.output_dir) / "predictions" / "predictions.jsonl"
        )

    if not predictions_path.exists():
        raise FileNotFoundError(
            f"Predictions file not found: {predictions_path}\n"
            "Please run 08_generate_samples.py first or specify predictions_path in config."
        )

    # Load predictions
    predictions = load_predictions(predictions_path)

    # Get metrics configuration
    metrics_config = cfg.evaluation.get("custom_metrics", {})
    compute_semantic = metrics_config.get("compute_semantic_similarity", True)
    compute_labels = metrics_config.get("compute_label_metrics", True)

    results = {}

    # Compute semantic similarity
    if compute_semantic:
        semantic_model = metrics_config.get(
            "semantic_model", "sentence-transformers/all-mpnet-base-v2"
        )
        results["semantic_similarity"] = compute_semantic_similarity(
            predictions, semantic_model
        )

    # Compute label metrics
    if compute_labels:
        extraction_model = metrics_config.get(
            "extraction_model", "Qwen/Qwen2.5-1.5B-Instruct"
        )
        extraction_device = metrics_config.get("extraction_device", "cuda")
        results["label_metrics"] = compute_label_metrics(
            predictions, extraction_model, extraction_device
        )

    # Save results
    output_dir = predictions_path.parent
    results_file = output_dir / "metrics_results.json"

    print("\n" + "=" * 50)
    print(f"Saving results to {results_file}")

    # Prepare results for JSON serialization (exclude per-sample details by default)
    summary_results = {
        "semantic_similarity": {
            k: v
            for k, v in results.get("semantic_similarity", {}).items()
            if k != "per_sample"  # Exclude large per-sample list from summary
        },
        "label_metrics": {
            "accuracy": results.get("label_metrics", {}).get("accuracy", {}),
        },
    }

    with open(results_file, "w") as f:
        json.dump(summary_results, f, indent=2)

    # Optionally save detailed results
    detailed_file = output_dir / "metrics_results_detailed.json"
    print(f"Saving detailed results to {detailed_file}")
    with open(detailed_file, "w") as f:
        json.dump(results, f, indent=2)

    print("=" * 50)
    print("METRICS COMPUTATION COMPLETE")
    print("=" * 50)

    # Print final summary
    print("\nFINAL SUMMARY:")
    if "semantic_similarity" in results:
        print(
            f"  Semantic Similarity (mean): {results['semantic_similarity']['mean']:.4f}"
        )
    if "label_metrics" in results:
        acc = results["label_metrics"]["accuracy"]
        print(f"  Label Accuracy (overall): {acc['overall']:.4f}")
        print(f"    - Stem Name: {acc['stem_name']:.4f}")
        print(f"    - Direction: {acc['direction']:.4f}")
        print(f"    - Magnitude: {acc['magnitude']:.4f}")


if __name__ == "__main__":
    main()
