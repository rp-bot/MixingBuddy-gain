"""
Compute custom evaluation metrics on saved predictions.

This script loads predictions from a JSONL file (generated by 08_generate_samples.py)
and computes:
1. Semantic similarity between ground truth and generated responses
2. Label extraction and accuracy (stem name, direction, magnitude)
"""

import hydra
from omegaconf import DictConfig
import sys
from pathlib import Path
import json
import gc
import torch
from typing import List, Dict
from tqdm import tqdm
import numpy as np

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.evaluation.semantic_similarity import SemanticSimilarityMetric  # noqa: E402
from src.evaluation.label_extraction import LabelExtractionMetric  # noqa: E402


def load_predictions(predictions_path: Path) -> List[Dict]:
    """Load predictions from JSONL file."""
    print(f"Loading predictions from {predictions_path}")
    predictions = []
    with open(predictions_path, "r") as f:
        for line in f:
            predictions.append(json.loads(line.strip()))
    print(f"Loaded {len(predictions)} predictions")
    return predictions


def compute_semantic_similarity(
    predictions: List[Dict],
    model_name: str,
) -> Dict:
    """
    Compute semantic similarity between ground truth and generated responses.

    Args:
        predictions: List of prediction dictionaries
        model_name: Name of the sentence transformer model

    Returns:
        Dictionary with mean, std, and per-sample similarities
    """
    print("\n" + "=" * 50)
    print("COMPUTING SEMANTIC SIMILARITY")
    print("=" * 50)

    metric = SemanticSimilarityMetric(model_name=model_name)

    similarities = []
    for pred in tqdm(predictions, desc="Computing similarities"):
        similarity = metric.compute(pred["ground_truth"], pred["generated"])
        similarities.append(similarity)

    results = {
        "mean": float(np.mean(similarities)),
        "std": float(np.std(similarities)),
        "median": float(np.median(similarities)),
        "min": float(np.min(similarities)),
        "max": float(np.max(similarities)),
        "per_sample": similarities,
    }

    print("\nSemantic Similarity Results:")
    print(f"  Mean: {results['mean']:.4f}")
    print(f"  Std:  {results['std']:.4f}")
    print(f"  Median: {results['median']:.4f}")
    print(f"  Min: {results['min']:.4f}")
    print(f"  Max: {results['max']:.4f}")

    # Clean up
    del metric
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return results


def compute_label_extraction(
    predictions: List[Dict],
    classifier_model: str,
    device: str,
) -> Dict:
    """
    Compute label extraction metrics on predictions.

    Args:
        predictions: List of prediction dictionaries
        classifier_model: Name of the zero-shot classification model
        device: Device to run model on (cuda/cpu)

    Returns:
        Dictionary with label extraction metrics
    """
    print(f"\nInitializing label extraction with classifier: {classifier_model}")
    label_metric = LabelExtractionMetric(
        classifier_model=classifier_model, device=device
    )
    label_results = label_metric.compute(predictions)

    # Clean up
    del label_metric
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return label_results


@hydra.main(config_path="../configs", config_name="03_evalutate_synthesis_instructions", version_base=None)
def main(cfg: DictConfig):
    """
    Main metrics computation function.
    """
    print("=" * 50)
    print("CUSTOM METRICS COMPUTATION")
    print("=" * 50)

    

    # Get predictions path - use the same run name as the checkpoint
    # If predictions_path is provided in config, use it; otherwise extract from checkpoint
    if hasattr(cfg, "predictions_path") and cfg.predictions_path:
        predictions_path = Path(cfg.predictions_path)
    else:
        # Extract run name from checkpoint path
        from src.utils.model_utils import find_latest_checkpoint

        checkpoint_path = cfg.checkpoint_path
        if checkpoint_path == "latest":
            checkpoint_path = find_latest_checkpoint()
            if checkpoint_path is None:
                raise ValueError(
                    "No checkpoint found when checkpoint_path is set to 'latest'"
                )

        # Extract run name from checkpoint path
        # Expected format: outputs/checkpoints/mixing_buddy_milestone_0/{run_name}/checkpoint-{step}
        checkpoint_path = Path(checkpoint_path)
        # Get the run name from the checkpoint path
        # It could be either the checkpoint path's name or its parent's name
        if checkpoint_path.name.startswith("checkpoint-"):
            run_name = checkpoint_path.parent.name
        else:
            run_name = checkpoint_path.name

        # Use the new evaluation structure
        predictions_path = (
            Path("outputs/evaluation") / run_name / "predictions" / "predictions.jsonl"
        )

    # Load predictions
    predictions = load_predictions(predictions_path)

    # Get semantic model from config
    metrics_config = cfg.evaluation.get("custom_metrics", {})
    semantic_model = metrics_config.get(
        "semantic_model", "sentence-transformers/all-mpnet-base-v2"
    )

    # Compute semantic similarity
    results = {
        "semantic_similarity": compute_semantic_similarity(predictions, semantic_model)
    }

    # Compute label extraction metrics with classifier
    classifier_model = metrics_config.get(
        "classifier_model", "MoritzLaurer/deberta-v3-base-zeroshot-v1"
    )
    device = "cuda"
    results["label_extraction"] = compute_label_extraction(
        predictions, classifier_model, device
    )

    # Save results
    output_dir = predictions_path.parent
    results_file = output_dir / "metrics_results.json"

    print("\n" + "=" * 50)
    print(f"Saving results to {results_file}")

    # Prepare results for JSON serialization (exclude per-sample details by default)
    summary_results = {
        "semantic_similarity": {
            k: v
            for k, v in results["semantic_similarity"].items()
            if k != "per_sample"  # Exclude large per-sample list from summary
        },
        "label_extraction": {
            k: v
            for k, v in results["label_extraction"].items()
            if k
            not in [
                "extracted_labels",
                "ground_truth_labels",
            ]  # Exclude large per-sample lists
        },
    }

    with open(results_file, "w") as f:
        json.dump(summary_results, f, indent=2)

    # Optionally save detailed results
    detailed_file = output_dir / "metrics_results_detailed.json"
    print(f"Saving detailed results to {detailed_file}")
    with open(detailed_file, "w") as f:
        json.dump(results, f, indent=2)

    print("=" * 50)
    print("METRICS COMPUTATION COMPLETE")
    print("=" * 50)

    # Print final summary
    print("\nFINAL SUMMARY:")
    print(f"  Semantic Similarity (mean): {results['semantic_similarity']['mean']:.4f}")

    if "label_extraction" in results:
        lex = results["label_extraction"]
        print(f"  Label Extraction (overall accuracy): {lex['overall_accuracy']:.4f}")
        print(f"    - Stem name: {lex['stem_name_accuracy']:.4f}")
        print(f"    - Magnitude range: {lex['magnitude_range_accuracy']:.4f}")
        print(
            f"    - Error detection: {lex['error_detection']['accuracy']:.4f} (F1: {lex['error_detection']['f1']:.4f})"
        )
        print(
            f"    - Problem severity: {lex['problem_severity']['accuracy']:.4f} (F1: {lex['problem_severity'].get('f1_macro', 0.0):.4f})"
        )
        print(
            f"    - Direction: {lex['direction']['accuracy']:.4f} (F1: {lex['direction'].get('f1_macro', 0.0):.4f})"
        )

    print("=" * 50)


if __name__ == "__main__":
    main()
